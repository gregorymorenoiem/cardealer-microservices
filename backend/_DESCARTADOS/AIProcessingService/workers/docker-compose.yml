# ============================================================
# Docker Compose - Auto-Learning Background Removal System
# ============================================================
#
# USO:
#   docker-compose up autolearn              # Procesar batch
#   docker-compose run autolearn --mode single --input /app/input
#   docker-compose run autolearn --mode continuous
#   docker-compose run autolearn --mode stats
#
# VOLÚMENES:
#   ./input   → /app/input    (imágenes a procesar)
#   ./output  → /app/output   (resultados)
#   ./models  → /app/models   (modelos descargados)
#   ./data    → /app/data     (base de datos SQLite)
#
# ============================================================

version: "3.8"

services:
  # ============================================================
  # Servicio principal de Auto-Learning
  # ============================================================
  autolearn:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: autolearn-system

    # Volúmenes para comunicación con el host
    volumes:
      # Carpeta de entrada (imágenes a procesar)
      - ./input:/app/input:ro

      # Carpeta de salida (resultados procesados)
      - ./output_autolearn:/app/output:rw

      # Modelos (SAM, YOLO, etc.) - persistente
      - ./models:/app/models:rw

      # Checkpoints del modelo de parámetros
      - ./checkpoints:/app/checkpoints:rw

      # Base de datos SQLite (persistente)
      - ./data:/app/data:rw

      # Checkpoint SAM (si existe en el host)
      - ./sam_vit_h_4b8939.pth:/app/sam_vit_h_4b8939.pth:ro

    # Variables de entorno
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
      - PYTHONUNBUFFERED=1
      - INPUT_DIR=/app/input
      - OUTPUT_DIR=/app/output
      - MODELS_DIR=/app/models
      - DB_PATH=/app/data/autolearn.db

    # Recursos
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G

    # Para acceder a Ollama en el host (macOS/Windows)
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Reiniciar en caso de fallo
    restart: unless-stopped

    # Comando por defecto (batch processing)
    command:
      ["--mode", "batch", "--input", "/app/input", "--output", "/app/output"]

  # ============================================================
  # Modo continuo (watch folder)
  # ============================================================
  autolearn-continuous:
    extends:
      service: autolearn
    container_name: autolearn-continuous
    command:
      [
        "--mode",
        "continuous",
        "--input",
        "/app/input",
        "--output",
        "/app/output",
      ]
    profiles:
      - continuous

  # ============================================================
  # Ollama (opcional - si quieres correrlo en Docker también)
  # ============================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    # Para GPU en Linux
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    profiles:
      - with-ollama

# ============================================================
# Volúmenes persistentes
# ============================================================
volumes:
  ollama_data:
    driver: local

# ============================================================
# Redes
# ============================================================
networks:
  default:
    name: autolearn-network
