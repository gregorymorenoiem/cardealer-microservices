# ============================================================================
# ChatbotService + LLM Server — Kubernetes Deployment
# ============================================================================
# Namespace: okla
# Requires: postgres, redis, rabbitmq services already running
#
# Apply:
#   kubectl apply -f k8s/chatbotservice.yaml -n okla
# ============================================================================

---
# ── ConfigMap ─────────────────────────────────────────────────────────────────
apiVersion: v1
kind: ConfigMap
metadata:
  name: chatbotservice-config
  namespace: okla
  labels:
    app: chatbotservice
data:
  LlmService__ServerUrl: "http://llm-server:8000"
  LlmService__TimeoutSeconds: "60"
  LlmService__Temperature: "0.3"
  LlmService__TopP: "0.9"
  LlmService__MaxTokens: "400"
  LlmService__RepetitionPenalty: "1.15"
  LlmService__CacheEnabled: "true"
  LlmService__CacheTtlMinutes: "30"
  Chatbot__DefaultMaxInteractionsPerSession: "10"
  Chatbot__SessionTimeoutMinutes: "30"
  Maintenance__EnableAutomatedTasks: "true"
  # R11 (MLOps): Feedback stored directly to PostgreSQL via ChatbotDbContext
  # No ephemeral JSONL files — all feedback persists in ChatMessages table
  Feedback__StorageType: "postgresql"
  Feedback__RetentionDays: "90"

---
# ── ChatbotService Deployment ────────────────────────────────────────────────
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chatbotservice
  namespace: okla
  labels:
    app: chatbotservice
    version: v1
  annotations:
    app.okla.com/component: "chatbot-backend"
    app.okla.com/managed-by: "github-actions"
    app.okla.com/pipeline: "chatbot-cicd.yml"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: chatbotservice
  template:
    metadata:
      labels:
        app: chatbotservice
        version: v1
    spec:
      imagePullSecrets:
        - name: registry-credentials
      containers:
        - name: chatbotservice
          image: ghcr.io/gregorymorenoiem/chatbotservice:latest
          ports:
            - containerPort: 8080
              protocol: TCP
          envFrom:
            - configMapRef:
                name: chatbotservice-config
          env:
            - name: ASPNETCORE_URLS
              value: "http://+:8080"
            - name: ASPNETCORE_ENVIRONMENT
              value: "Production"
            - name: ConnectionStrings__DefaultConnection
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: chatbot-db-connection
            - name: Jwt__Key
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: jwt-secret-key
            - name: Redis__ConnectionString
              value: "redis:6379,password=$(REDIS_PASSWORD)"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-secrets
                  key: REDIS_PASSWORD
            - name: RabbitMQ__Host
              value: "rabbitmq"
            - name: RabbitMQ__Username
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: rabbitmq-user
            - name: RabbitMQ__Password
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: rabbitmq-password
            - name: WhatsApp__VerifyToken
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: whatsapp-verify-token
            - name: WhatsApp__AccessToken
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: whatsapp-access-token
            - name: WhatsApp__PhoneNumberId
              valueFrom:
                secretKeyRef:
                  name: okla-secrets
                  key: whatsapp-phone-number-id
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 12

---
# ── ChatbotService Service ───────────────────────────────────────────────────
apiVersion: v1
kind: Service
metadata:
  name: chatbotservice
  namespace: okla
  labels:
    app: chatbotservice
spec:
  selector:
    app: chatbotservice
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: ClusterIP

---
# ── LLM Server Deployment ────────────────────────────────────────────────────
# NOTE: LLM Server is CPU-intensive. Use dedicated node pool if available.
# DISABLED: Set replicas to 0 until image is built and pushed to GHCR,
# and a node pool with sufficient resources (2+ CPU, 6Gi+ RAM) is available.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
  namespace: okla
  labels:
    app: llm-server
    version: v1
  annotations:
    app.okla.com/component: "llm-inference"
    app.okla.com/managed-by: "github-actions"
    app.okla.com/pipeline: "chatbot-cicd.yml"
    app.okla.com/model: "okla-llama3-8b-q4_k_m"
spec:
  replicas: 0 # Disabled: image not in GHCR + insufficient node resources
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # Spin up new pod BEFORE killing old one
      maxUnavailable: 0 # Zero-downtime: old pod stays until new is ready
  selector:
    matchLabels:
      app: llm-server
  template:
    metadata:
      labels:
        app: llm-server
        version: v1
    spec:
      imagePullSecrets:
        - name: registry-credentials
      containers:
        - name: llm-server
          image: ghcr.io/gregorymorenoiem/llm-server:latest
          ports:
            - containerPort: 8000
              protocol: TCP
          env:
            - name: MODEL_PATH
              value: "/models/okla-llama3-8b-q4_k_m.gguf"
            - name: HOST
              value: "0.0.0.0"
            - name: PORT
              value: "8000"
            - name: N_CTX
              value: "8192"
            - name: N_GPU_LAYERS
              value: "0"
            - name: N_THREADS
              value: "4"
            - name: MAX_TOKENS
              value: "600"
            - name: TEMPERATURE
              value: "0.3"
            - name: TOP_P
              value: "0.9"
            - name: REPETITION_PENALTY
              value: "1.15"
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "1000m"
              memory: "2Gi"
          volumeMounts:
            - name: model-storage
              mountPath: /models
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120 # Model loading takes time
            periodSeconds: 60
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 5
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 30 # Up to 5 min for model loading
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: llm-model-pvc

---
# ── LLM Server Service ───────────────────────────────────────────────────────
apiVersion: v1
kind: Service
metadata:
  name: llm-server
  namespace: okla
  labels:
    app: llm-server
spec:
  selector:
    app: llm-server
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
  type: ClusterIP

---
# ── PVC for Model Storage ────────────────────────────────────────────────────
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-model-pvc
  namespace: okla
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: do-block-storage

---
# ── PodDisruptionBudgets ─────────────────────────────────────────────────────
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: chatbotservice-pdb
  namespace: okla
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: chatbotservice

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: llm-server-pdb
  namespace: okla
spec:
  maxUnavailable: 0
  selector:
    matchLabels:
      app: llm-server
