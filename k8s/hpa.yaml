# =============================================================================
# HORIZONTAL POD AUTOSCALER (HPA) â€” Arquitectura Auto-Escalable
# =============================================================================
# Auto-scaling basado en CPU + Memory para TODOS los microservicios.
#
# TIERS DE ESCALADO:
#   ðŸ”´ CRITICAL: minReplicas=2, maxReplicas=8, CPU 50%, Memory 70%
#   ðŸŸ  IMPORTANT: minReplicas=1, maxReplicas=6, CPU 60%, Memory 75%
#   ðŸŸ¡ STANDARD:  minReplicas=1, maxReplicas=4, CPU 70%, Memory 80%
#   ðŸ”µ INTERNAL:  minReplicas=1, maxReplicas=3, CPU 75%
#
# SCALE-UP:  Agresivo â€” max 3 pods cada 30s (respuesta rÃ¡pida a trÃ¡fico)
# SCALE-DOWN: Conservador â€” max 1 pod cada 60s, stabilization 300s
#
# Para mÃ©tricas custom (request rate, queue depth) ver keda.yaml
# =============================================================================

---
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ”´ TIER CRITICAL â€” Alta disponibilidad, escalado agresivo
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Frontend Web â€” Entry point, escala por CPU
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-web-hpa
  namespace: okla
  labels:
    tier: critical
    app.kubernetes.io/part-of: okla
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend-web
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Pods
          value: 3
          periodSeconds: 30
        - type: Percent
          value: 100
          periodSeconds: 30
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# Gateway â€” Punto de entrada API, bottleneck principal
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gateway-hpa
  namespace: okla
  labels:
    tier: critical
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gateway
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Pods
          value: 3
          periodSeconds: 30
        - type: Percent
          value: 100
          periodSeconds: 30
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# AuthService â€” AutenticaciÃ³n, cada request pasa por aquÃ­
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: authservice-hpa
  namespace: okla
  labels:
    tier: critical
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: authservice
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Pods
          value: 3
          periodSeconds: 30
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# VehiclesSaleService â€” Core business, bÃºsquedas y catÃ¡logo
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vehiclessaleservice-hpa
  namespace: okla
  labels:
    tier: critical
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vehiclessaleservice
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Pods
          value: 3
          periodSeconds: 30
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# âš ï¸ BillingService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸŸ  TIER IMPORTANT â€” Escalado moderado
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# âš ï¸ UserService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# RoleService â€” AutorizaciÃ³n, requerido para todos los requests
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: roleservice-hpa
  namespace: okla
  labels:
    tier: important
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: roleservice
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# âš ï¸ MediaService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# âš ï¸ NotificationService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# KYCService â€” VerificaciÃ³n de identidad
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kycservice-hpa
  namespace: okla
  labels:
    tier: important
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kycservice
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# ContactService â€” Contacto entre compradores y vendedores
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: contactservice-hpa
  namespace: okla
  labels:
    tier: important
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: contactservice
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60

---
# AdminService â€” Panel admin
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: adminservice-hpa
  namespace: okla
  labels:
    tier: important
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: adminservice
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 65
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120

---
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ”µ TIER INTERNAL â€” Servicios de soporte, escalado conservador
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# âš ï¸ ErrorService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# âš ï¸ AuditService â€” Gestionado por KEDA (ver keda.yaml)
# No definir HPA aquÃ­ para evitar conflicto de escaladores duales.

---
# IdempotencyService â€” Control de duplicados
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: idempotencyservice-hpa
  namespace: okla
  labels:
    tier: internal
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: idempotencyservice
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300

---
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ¤– TIER AI/ML â€” Chatbot & LLM inference
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ChatbotService â€” .NET 8 API backend (lightweight, scales like standard)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: chatbotservice-hpa
  namespace: okla
  labels:
    tier: ai-ml
    app.kubernetes.io/part-of: okla-chatbot
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chatbotservice
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120

---
# LLM Server â€” CPU-intensive inference (conservative scaling, high memory)
# âš ï¸ Each replica loads full GGUF model (~4.7 GB RAM + inference overhead)
# Scale conservatively: memory is the bottleneck, not CPU
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-server-hpa
  namespace: okla
  labels:
    tier: ai-ml
    app.kubernetes.io/part-of: okla-chatbot
  annotations:
    app.okla.com/note: "Conservative scaling â€” each pod loads 4.7GB model"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-server
  minReplicas: 1
  maxReplicas: 2 # Max 2 â€” each uses 6-8Gi RAM. Evaluate GPU before scaling >2.
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300 # 5 min â€” model loading takes time
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300 # Max 1 new pod every 5 min
    scaleDown:
      stabilizationWindowSeconds: 600 # 10 min â€” avoid thrashing
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300
