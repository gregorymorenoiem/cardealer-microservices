# üî¥ Post-Mortem: Deploy Febrero 2026 ‚Äî ¬øPor qu√© fallaron 6 cosas si hubo 15 auditor√≠as?

**Proyecto:** OKLA (CarDealer Microservices)  
**Fecha del deploy:** Febrero 17‚Äì18, 2026  
**Autor:** An√°lisis post-mortem automatizado  
**Clasificaci√≥n:** An√°lisis de brechas entre auditor√≠as y operaciones reales

---

## üìã Resumen Ejecutivo

El deploy a Digital Ocean Kubernetes (DOKS) present√≥ **6 fallos cr√≠ticos** que requirieron intervenci√≥n manual durante varias horas. A pesar de contar con **15 auditor√≠as realizadas** por especialistas diversos, **ninguna** de las 15 detect√≥ ni previno estos problemas.

**La causa ra√≠z no es que las auditor√≠as fallaron.** La causa ra√≠z es que **nunca se audit√≥ la capa de infraestructura y deployment.** Todas las auditor√≠as cubrieron c√≥digo de aplicaci√≥n, arquitectura, IA, frontend y procesos de negocio ‚Äî pero ninguna cubri√≥ Dockerfiles, CI/CD, Kubernetes manifests, ni el pipeline de deployment.

---

## üî• Los 6 Fallos del Deploy

### Fallo 1: AuthService ‚Äî Readiness Probe Timeout (200 segundos)

| Detalle        | Valor                                                                                                                                                                   |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **S√≠ntoma**    | Pod `authservice` nunca pasaba readiness check, K8s lo mataba y reiniciaba en loop                                                                                      |
| **Causa ra√≠z** | `ExternalServiceHealthCheck` intentaba conectar a Consul en `localhost:8500` (no desplegado). Timeout de 200s bloqueaba el thread pool, matando tambi√©n `/health/ready` |
| **Fix**        | Excluir checks con tag `"external"` del endpoint `/health` en `Program.cs`                                                                                              |
| **Archivo**    | `AuthService.Api/Program.cs` l√≠nea 331                                                                                                                                  |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ö†Ô∏è **Parcialmente.** La auditor√≠a de Observabilidad (#6) list√≥ los health endpoints de cada servicio, pero **NO ejecut√≥** los health checks para ver si funcionaban. Solo verific√≥ que exist√≠an en el c√≥digo.
- ‚ùå Nadie verific√≥ que Consul no estaba desplegado en K8s.
- ‚ùå Nadie hizo un `kubectl exec` ‚Üí `curl /health` para validar tiempos de respuesta.

---

### Fallo 2: Registry Credentials Expiradas (ImagePullBackOff)

| Detalle        | Valor                                                                                                                    |
| -------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **S√≠ntoma**    | Pods nuevos no pod√≠an bajar im√°genes de GHCR ‚Äî `ImagePullBackOff` con HTTP 403                                           |
| **Causa ra√≠z** | El K8s secret `registry-credentials` conten√≠a un token `ghs_*` ef√≠mero de GitHub Actions que expir√≥ despu√©s del workflow |
| **Fix**        | Recrear secret con token `gho_*` OAuth de `gh auth token`                                                                |
| **Archivo**    | Secret K8s `registry-credentials`                                                                                        |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ùå **Ninguna.** Cero auditor√≠as revisaron secrets de Kubernetes, pol√≠ticas de rotaci√≥n de tokens, ni el flujo de autenticaci√≥n al container registry.

---

### Fallo 3: IDeadLetterQueue ‚Äî DI Crash en 6 Servicios

| Detalle        | Valor                                                                                                                                                                                                            |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **S√≠ntoma**    | 6 servicios crasheaban al iniciar: `Unable to resolve service for type 'IDeadLetterQueue'`                                                                                                                       |
| **Causa ra√≠z** | `AddPostgreSqlDeadLetterQueue()` registra `ISharedDeadLetterQueue` (de CarDealer.Shared), pero `DeadLetterQueueProcessor` depende de `IDeadLetterQueue` (interfaz local). Mismatch de interfaces en DI container |
| **Fix**        | Agregar `AddSingleton<IDeadLetterQueue, InMemoryDeadLetterQueue>()` a los 6 servicios                                                                                                                            |
| **Archivos**   | `Program.cs` de Auth, Error, Role, Audit, Notification, Media services                                                                                                                                           |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ùå **Ninguna.** Cero auditor√≠as revisaron el wiring de Dependency Injection en `Program.cs`.
- ‚ùå La auditor√≠a de Observabilidad (#6) evalu√≥ patrones de arquitectura pero NO ejecut√≥ los servicios para verificar que arrancan.
- ‚ùå La auditor√≠a de Testing (#6) report√≥ cobertura de 52/100 ‚Äî ning√∫n test de integraci√≥n validaba el startup del DI container.

---

### Fallo 4: Docker Build Cache Envenenado

| Detalle        | Valor                                                                                                                                                                                                                  |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **S√≠ntoma**    | CI/CD reportaba builds exitosos, pero las im√°genes Docker ten√≠an el mismo digest (c√≥digo viejo). Los pods segu√≠an corriendo c√≥digo anterior                                                                            |
| **Causa ra√≠z** | `_reusable-dotnet-service.yml` usa `cache-from: type=local` con `restore-keys` pattern. Buildx reutilizaba ALL cached layers incluyendo `COPY . .` y `dotnet publish`, produciendo im√°genes id√©nticas a las anteriores |
| **Fix**        | Eliminar todos los caches `Linux-buildx-*` via `gh cache delete`                                                                                                                                                       |
| **Archivo**    | `.github/workflows/_reusable-dotnet-service.yml` l√≠neas 187-196                                                                                                                                                        |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ùå **Ninguna.** Cero auditor√≠as revisaron los workflows de GitHub Actions, la estrategia de cache de Docker, ni la configuraci√≥n de buildx.
- ‚ùå Ni siquiera la auditor√≠a MLOps (#10), que cre√≥ un workflow CI/CD para el chatbot, revis√≥ los workflows existentes de los otros 13 servicios.

---

### Fallo 5: RabbitMQ Queue PRECONDITION_FAILED

| Detalle        | Valor                                                                                                                                                                                                                             |
| -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **S√≠ntoma**    | NotificationService crasheaba con `PRECONDITION_FAILED` al declarar queues                                                                                                                                                        |
| **Causa ra√≠z** | Las queues existentes en RabbitMQ fueron creadas sin `x-dead-letter-exchange`. El c√≥digo nuevo las declara CON `x-dead-letter-exchange: notification-exchange.dlx`. RabbitMQ no permite cambiar argumentos de una queue existente |
| **Fix**        | Eliminar las 6 queues existentes via `rabbitmqctl delete_queue` para que el c√≥digo las recreara con los argumentos correctos                                                                                                      |
| **Archivo**    | Queue topology en RabbitMQ (runtime, no c√≥digo)                                                                                                                                                                                   |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ùå **Ninguna.** Cero auditor√≠as revisaron la topolog√≠a de RabbitMQ (exchanges, queues, bindings, argumentos).
- ‚ö†Ô∏è La auditor√≠a de Observabilidad (#6) mencion√≥ "ErrorService DLQ" y "RabbitMQ health check ausente" pero NO audit√≥ la configuraci√≥n real de las queues ni sus argumentos.
- ‚ùå No existe proceso de migraci√≥n de queues (an√°logo a EF Core migrations para DB).

---

### Fallo 6: Frontend Image Name Mismatch

| Detalle        | Valor                                                                                                                                                                                      |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **S√≠ntoma**    | `okla.com.do` mostraba la p√°gina vieja (Vite/SPA con "cardealer.do") en vez del Next.js nuevo                                                                                              |
| **Causa ra√≠z** | K8s `deployments.yaml` referenciaba `cardealer-web:latest` (imagen vieja), pero el CI/CD workflow construye y pushea `frontend-web:latest` (imagen nueva). Mismatch en el nombre de imagen |
| **Fix**        | `kubectl set image` + actualizar `deployments.yaml` l√≠nea 50                                                                                                                               |
| **Archivo**    | `k8s/deployments.yaml` l√≠nea 50                                                                                                                                                            |

**¬øAlguna auditor√≠a lo cubr√≠a?**

- ‚ùå **Ninguna.** Cero auditor√≠as compararon los nombres de imagen en `deployments.yaml` contra los nombres en los workflows de CI/CD.
- ‚ùå La auditor√≠a de Gateway (#5) revis√≥ rutas Ocelot pero NO los manifests de K8s.
- ‚ùå No existe validaci√≥n automatizada que compare `deployments.yaml` ‚Üî `_reusable-frontend.yml` ‚Üî GHCR registry.

---

## üìä Matriz: Auditor√≠as vs Fallos

| Auditor√≠a              | Scope               | F1: Health | F2: Registry | F3: DI | F4: Cache | F5: RabbitMQ | F6: Image |
| ---------------------- | ------------------- | :--------: | :----------: | :----: | :-------: | :----------: | :-------: |
| #1 Model Architect     | ChatbotService arch |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #2 AI Researcher       | LLM pipeline        |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #3 Frontend Auditor    | React/TSX code      |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #4 Roles & Security    | RBAC flows          |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #5 Gateway Auditor     | Ocelot routes       |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #6 Standards & Observ. | Code quality        | ‚ö†Ô∏è Parcial |      ‚ùå      |   ‚ùå   |    ‚ùå     |  ‚ö†Ô∏è Parcial  |    ‚ùå     |
| #7 Business Coverage   | Processes           |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #8 API Documentation   | Endpoint docs       |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #9 Conversational AI   | Chatbot dialogue    |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |
| #10 MLOps Engineer     | ML operations       |     ‚ùå     |      ‚ùå      |   ‚ùå   |    ‚ùå     |      ‚ùå      |    ‚ùå     |

**Resultado: 0 de 6 fallos fueron detectados por alguna auditor√≠a.**

---

## üîç An√°lisis: ¬øPor Qu√© Pas√≥ Esto?

### 1. Sesgo hacia c√≥digo de aplicaci√≥n, ceguera hacia infraestructura

Las 15 auditor√≠as se distribuyeron as√≠:

```
C√≥digo de Aplicaci√≥n (.cs, .tsx):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 10 auditor√≠as (67%)
IA / Machine Learning:             ‚ñà‚ñà‚ñà‚ñà           4 auditor√≠as (27%)
Infraestructura (Docker/K8s/CI):   ‚ñë              0 auditor√≠as (0%)  ‚Üê BRECHA
```

**El 100% del esfuerzo de auditor√≠a se concentr√≥ en "¬øel c√≥digo est√° bien escrito?"** pero nadie pregunt√≥ **"¬øel c√≥digo llega correctamente a producci√≥n?"**

### 2. Auditor√≠as est√°ticas vs verificaci√≥n din√°mica

Todas las auditor√≠as fueron **an√°lisis est√°tico de archivos**:

- Leer c√≥digo ‚Üí opinar sobre patrones
- Leer configs ‚Üí verificar completitud
- Contar tests ‚Üí medir cobertura

**Ninguna auditor√≠a ejecut√≥ el sistema:**

- ‚ùå Nadie corri√≥ `docker build` para verificar que la imagen se construye
- ‚ùå Nadie corri√≥ `docker compose up` para verificar que los servicios arrancan
- ‚ùå Nadie despleg√≥ en un cluster de prueba
- ‚ùå Nadie verific√≥ que los health checks responden < 10s
- ‚ùå Nadie verific√≥ que las queues de RabbitMQ son compatibles entre versiones

### 3. Falta de un especialista de infraestructura

| Capa del sistema           | Especialista asignado | Estado             |
| -------------------------- | --------------------- | ------------------ |
| Chatbot / IA               | 4 especialistas       | ‚úÖ Ultra-cubierto  |
| Frontend React             | 1 especialista        | ‚úÖ Cubierto        |
| Backend .NET (c√≥digo)      | 3 especialistas       | ‚úÖ Cubierto        |
| Business processes         | 1 especialista        | ‚úÖ Cubierto        |
| **Dockerfiles**            | **Ninguno**           | üî¥ **NO CUBIERTO** |
| **CI/CD Workflows**        | **Ninguno**           | üî¥ **NO CUBIERTO** |
| **K8s Manifests**          | **Ninguno**           | üî¥ **NO CUBIERTO** |
| **Messaging (RabbitMQ)**   | **Ninguno**           | üî¥ **NO CUBIERTO** |
| **DI Wiring (Program.cs)** | **Ninguno**           | üî¥ **NO CUBIERTO** |
| **Secrets & Credentials**  | **Ninguno**           | üî¥ **NO CUBIERTO** |

### 4. La falsa sensaci√≥n de seguridad

Con puntuaciones de **9.2/10**, **9.3/10**, **9.0/10** en m√∫ltiples auditor√≠as, se cre√≥ una percepci√≥n de que el sistema estaba "listo para producci√≥n". Pero estas puntuaciones solo reflejan la calidad del **c√≥digo de aplicaci√≥n**, no la madurez del **pipeline de deployment**.

Es como auditar que un avi√≥n tiene excelentes motores, alas y avi√≥nica ‚Äî pero nadie revis√≥ si la pista de aterrizaje existe.

---

## üéØ Los 6 Fallos Categorizados

| Fallo                    | Categor√≠a       | ¬øSe detecta con test unitario? | ¬øSe detecta con an√°lisis est√°tico? | ¬øC√≥mo se detecta?                 |
| ------------------------ | --------------- | :----------------------------: | :--------------------------------: | --------------------------------- |
| F1: Health check timeout | Infra + Config  |               ‚ùå               |                 ‚ùå                 | Smoke test en cluster real        |
| F2: Registry credentials | Secrets/DevOps  |               ‚ùå               |                 ‚ùå                 | Checklist de rotaci√≥n de secrets  |
| F3: DI mismatch          | Wiring/Startup  |    ‚úÖ Con integration test     |             ‚ö†Ô∏è Dif√≠cil             | `WebApplicationFactory` test      |
| F4: Build cache poison   | CI/CD           |               ‚ùå               |                 ‚ùå                 | Verificar image digest post-build |
| F5: RabbitMQ args        | Messaging infra |               ‚ùå               |                 ‚ùå                 | Queue migration strategy          |
| F6: Image name mismatch  | K8s ‚Üî CI/CD     |               ‚ùå               |        ‚úÖ Con linter de K8s        | Cross-reference YAML validation   |

**Solo 1 de 6 fallos (F3) se podr√≠a haber detectado con tests unitarios convencionales.** Los otros 5 requieren validaci√≥n de infraestructura que no estaba en el scope de ninguna auditor√≠a.

---

## ‚úÖ Recomendaciones: Auditor√≠as Faltantes

### üî¥ P0 ‚Äî Auditor√≠a de Infraestructura y Deployment (URGENTE)

Esta es la auditor√≠a que falt√≥ y habr√≠a prevenido los 6 fallos:

| √Årea                   | Qu√© auditar                                                                                                |
| ---------------------- | ---------------------------------------------------------------------------------------------------------- |
| **Dockerfiles**        | Multi-stage builds, base images, cacheo de layers, puertos, healthchecks                                   |
| **CI/CD Workflows**    | Build triggers, cache strategy, image naming, push conditions, deploy gates                                |
| **K8s Manifests**      | Image names vs CI/CD, resource limits, probes, secrets, PVCs                                               |
| **DI Startup**         | Integration tests con `WebApplicationFactory` que validen que el DI container resuelve todos los servicios |
| **RabbitMQ Topology**  | Queue arguments, exchange bindings, DLX config, migration strategy                                         |
| **Secrets Management** | Rotaci√≥n de tokens, tipos de tokens (ephemeral vs long-lived), expiraci√≥n                                  |
| **Smoke Tests**        | Script que haga `curl /health` a cada servicio despu√©s del deploy                                          |
| **Image Consistency**  | Validar que `deployments.yaml` referencia las mismas im√°genes que CI/CD construye                          |

### üü° P1 ‚Äî Validaci√≥n Automatizada en CI/CD

```yaml
# Propuesta: Job de validaci√≥n pre-deploy
validate-manifests:
  steps:
    # 1. Verificar que image names en K8s coinciden con CI/CD
    - name: Cross-reference image names
      run: |
        for svc in frontend-web gateway authservice ...; do
          K8S_IMAGE=$(grep "image:.*$svc" k8s/deployments.yaml)
          CI_IMAGE=$(grep "tags:.*$svc" .github/workflows/*.yml)
          # Compare and fail if mismatch
        done

    # 2. Verificar DI startup de cada servicio
    - name: DI smoke test
      run: dotnet test --filter "Category=Startup"

    # 3. Verificar que Dockerfiles buildean
    - name: Docker build validation
      run: docker build --target runner -t test ./backend/AuthService
```

### üü¢ P2 ‚Äî Startup Integration Tests

```csharp
// Test que habr√≠a detectado Fallo #3 (IDeadLetterQueue DI crash)
[Fact]
[Trait("Category", "Startup")]
public async Task Application_Starts_Successfully()
{
    await using var app = new WebApplicationFactory<Program>();
    using var client = app.CreateClient();
    var response = await client.GetAsync("/health");
    response.EnsureSuccessStatusCode();
}
```

---

## üìà Lecci√≥n Aprendida

> **"Un sistema no est√° listo para producci√≥n cuando el c√≥digo est√° bien escrito.
> Est√° listo cuando el c√≥digo llega a producci√≥n de forma confiable y repetible."**

Las auditor√≠as existentes aseguran que el software es de calidad. Lo que falt√≥ es asegurar que el **camino del c√≥digo al usuario** (build ‚Üí push ‚Üí deploy ‚Üí run) funciona sin fricci√≥n.

### Distribuci√≥n ideal de auditor√≠as para un proyecto con 86 microservicios:

```
Actual:                              Ideal:
C√≥digo: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 67%           C√≥digo: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 40%
IA/ML:  ‚ñà‚ñà‚ñà‚ñà           27%           IA/ML:  ‚ñà‚ñà‚ñà‚ñà     20%
Infra:  ‚ñë               0%    ‚Üí      Infra:  ‚ñà‚ñà‚ñà‚ñà     20%  ‚Üê NUEVA
DevOps:                  0%           DevOps: ‚ñà‚ñà‚ñà      15%  ‚Üê NUEVA
E2E:                     0%           E2E:    ‚ñà         5%  ‚Üê NUEVA
```

---

## üìä Resumen Final

| M√©trica                                   | Valor                               |
| ----------------------------------------- | ----------------------------------- |
| Total de auditor√≠as realizadas            | 15                                  |
| Auditor√≠as que cubr√≠an infraestructura    | **0**                               |
| Fallos en el deploy                       | **6**                               |
| Fallos prevenibles con auditor√≠a de infra | **6 (100%)**                        |
| Tiempo total de resoluci√≥n                | ~6 horas                            |
| Costo de oportunidad                      | Alto (downtime en staging)          |
| Lecci√≥n                                   | Auditar c√≥digo ‚â† Auditar deployment |

---

_Post-mortem generado el 18 de febrero de 2026_  
_Proyecto OKLA ‚Äî Deploy Staging DOKS_
