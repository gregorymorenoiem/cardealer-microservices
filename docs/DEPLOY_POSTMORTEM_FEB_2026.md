# ğŸ”´ Post-Mortem: Deploy Febrero 2026 â€” Â¿Por quÃ© fallaron 6 cosas si hubo 15 auditorÃ­as?

**Proyecto:** OKLA (CarDealer Microservices)  
**Fecha del deploy:** Febrero 17â€“18, 2026  
**Autor:** AnÃ¡lisis post-mortem automatizado  
**ClasificaciÃ³n:** AnÃ¡lisis de brechas entre auditorÃ­as y operaciones reales

---

## ğŸ“‹ Resumen Ejecutivo

El deploy a Digital Ocean Kubernetes (DOKS) presentÃ³ **6 fallos crÃ­ticos** que requirieron intervenciÃ³n manual durante varias horas. A pesar de contar con **15 auditorÃ­as realizadas** por especialistas diversos, **ninguna** de las 15 detectÃ³ ni previno estos problemas.

**La causa raÃ­z no es que las auditorÃ­as fallaron.** La causa raÃ­z es que **nunca se auditÃ³ la capa de infraestructura y deployment.** Todas las auditorÃ­as cubrieron cÃ³digo de aplicaciÃ³n, arquitectura, IA, frontend y procesos de negocio â€” pero ninguna cubriÃ³ Dockerfiles, CI/CD, Kubernetes manifests, ni el pipeline de deployment.

---

## ğŸ”¥ Los 6 Fallos del Deploy

### Fallo 1: AuthService â€” Readiness Probe Timeout (200 segundos)

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | Pod `authservice` nunca pasaba readiness check, K8s lo mataba y reiniciaba en loop |
| **Causa raÃ­z** | `ExternalServiceHealthCheck` intentaba conectar a Consul en `localhost:8500` (no desplegado). Timeout de 200s bloqueaba el thread pool, matando tambiÃ©n `/health/ready` |
| **Fix** | Excluir checks con tag `"external"` del endpoint `/health` en `Program.cs` |
| **Archivo** | `AuthService.Api/Program.cs` lÃ­nea 331 |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âš ï¸ **Parcialmente.** La auditorÃ­a de Observabilidad (#6) listÃ³ los health endpoints de cada servicio, pero **NO ejecutÃ³** los health checks para ver si funcionaban. Solo verificÃ³ que existÃ­an en el cÃ³digo.
- âŒ Nadie verificÃ³ que Consul no estaba desplegado en K8s.
- âŒ Nadie hizo un `kubectl exec` â†’ `curl /health` para validar tiempos de respuesta.

---

### Fallo 2: Registry Credentials Expiradas (ImagePullBackOff)

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | Pods nuevos no podÃ­an bajar imÃ¡genes de GHCR â€” `ImagePullBackOff` con HTTP 403 |
| **Causa raÃ­z** | El K8s secret `registry-credentials` contenÃ­a un token `ghs_*` efÃ­mero de GitHub Actions que expirÃ³ despuÃ©s del workflow |
| **Fix** | Recrear secret con token `gho_*` OAuth de `gh auth token` |
| **Archivo** | Secret K8s `registry-credentials` |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âŒ **Ninguna.** Cero auditorÃ­as revisaron secrets de Kubernetes, polÃ­ticas de rotaciÃ³n de tokens, ni el flujo de autenticaciÃ³n al container registry.

---

### Fallo 3: IDeadLetterQueue â€” DI Crash en 6 Servicios

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | 6 servicios crasheaban al iniciar: `Unable to resolve service for type 'IDeadLetterQueue'` |
| **Causa raÃ­z** | `AddPostgreSqlDeadLetterQueue()` registra `ISharedDeadLetterQueue` (de CarDealer.Shared), pero `DeadLetterQueueProcessor` depende de `IDeadLetterQueue` (interfaz local). Mismatch de interfaces en DI container |
| **Fix** | Agregar `AddSingleton<IDeadLetterQueue, InMemoryDeadLetterQueue>()` a los 6 servicios |
| **Archivos** | `Program.cs` de Auth, Error, Role, Audit, Notification, Media services |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âŒ **Ninguna.** Cero auditorÃ­as revisaron el wiring de Dependency Injection en `Program.cs`.
- âŒ La auditorÃ­a de Observabilidad (#6) evaluÃ³ patrones de arquitectura pero NO ejecutÃ³ los servicios para verificar que arrancan.
- âŒ La auditorÃ­a de Testing (#6) reportÃ³ cobertura de 52/100 â€” ningÃºn test de integraciÃ³n validaba el startup del DI container.

---

### Fallo 4: Docker Build Cache Envenenado

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | CI/CD reportaba builds exitosos, pero las imÃ¡genes Docker tenÃ­an el mismo digest (cÃ³digo viejo). Los pods seguÃ­an corriendo cÃ³digo anterior |
| **Causa raÃ­z** | `_reusable-dotnet-service.yml` usa `cache-from: type=local` con `restore-keys` pattern. Buildx reutilizaba ALL cached layers incluyendo `COPY . .` y `dotnet publish`, produciendo imÃ¡genes idÃ©nticas a las anteriores |
| **Fix** | Eliminar todos los caches `Linux-buildx-*` via `gh cache delete` |
| **Archivo** | `.github/workflows/_reusable-dotnet-service.yml` lÃ­neas 187-196 |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âŒ **Ninguna.** Cero auditorÃ­as revisaron los workflows de GitHub Actions, la estrategia de cache de Docker, ni la configuraciÃ³n de buildx.
- âŒ Ni siquiera la auditorÃ­a MLOps (#10), que creÃ³ un workflow CI/CD para el chatbot, revisÃ³ los workflows existentes de los otros 13 servicios.

---

### Fallo 5: RabbitMQ Queue PRECONDITION_FAILED

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | NotificationService crasheaba con `PRECONDITION_FAILED` al declarar queues |
| **Causa raÃ­z** | Las queues existentes en RabbitMQ fueron creadas sin `x-dead-letter-exchange`. El cÃ³digo nuevo las declara CON `x-dead-letter-exchange: notification-exchange.dlx`. RabbitMQ no permite cambiar argumentos de una queue existente |
| **Fix** | Eliminar las 6 queues existentes via `rabbitmqctl delete_queue` para que el cÃ³digo las recreara con los argumentos correctos |
| **Archivo** | Queue topology en RabbitMQ (runtime, no cÃ³digo) |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âŒ **Ninguna.** Cero auditorÃ­as revisaron la topologÃ­a de RabbitMQ (exchanges, queues, bindings, argumentos).
- âš ï¸ La auditorÃ­a de Observabilidad (#6) mencionÃ³ "ErrorService DLQ" y "RabbitMQ health check ausente" pero NO auditÃ³ la configuraciÃ³n real de las queues ni sus argumentos.
- âŒ No existe proceso de migraciÃ³n de queues (anÃ¡logo a EF Core migrations para DB).

---

### Fallo 6: Frontend Image Name Mismatch

| Detalle | Valor |
|---------|-------|
| **SÃ­ntoma** | `okla.com.do` mostraba la pÃ¡gina vieja (Vite/SPA con "cardealer.do") en vez del Next.js nuevo |
| **Causa raÃ­z** | K8s `deployments.yaml` referenciaba `cardealer-web:latest` (imagen vieja), pero el CI/CD workflow construye y pushea `frontend-web:latest` (imagen nueva). Mismatch en el nombre de imagen |
| **Fix** | `kubectl set image` + actualizar `deployments.yaml` lÃ­nea 50 |
| **Archivo** | `k8s/deployments.yaml` lÃ­nea 50 |

**Â¿Alguna auditorÃ­a lo cubrÃ­a?**
- âŒ **Ninguna.** Cero auditorÃ­as compararon los nombres de imagen en `deployments.yaml` contra los nombres en los workflows de CI/CD.
- âŒ La auditorÃ­a de Gateway (#5) revisÃ³ rutas Ocelot pero NO los manifests de K8s.
- âŒ No existe validaciÃ³n automatizada que compare `deployments.yaml` â†” `_reusable-frontend.yml` â†” GHCR registry.

---

## ğŸ“Š Matriz: AuditorÃ­as vs Fallos

| AuditorÃ­a | Scope | F1: Health | F2: Registry | F3: DI | F4: Cache | F5: RabbitMQ | F6: Image |
|-----------|-------|:----------:|:------------:|:------:|:---------:|:------------:|:---------:|
| #1 Model Architect | ChatbotService arch | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #2 AI Researcher | LLM pipeline | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #3 Frontend Auditor | React/TSX code | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #4 Roles & Security | RBAC flows | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #5 Gateway Auditor | Ocelot routes | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #6 Standards & Observ. | Code quality | âš ï¸ Parcial | âŒ | âŒ | âŒ | âš ï¸ Parcial | âŒ |
| #7 Business Coverage | Processes | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #8 API Documentation | Endpoint docs | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #9 Conversational AI | Chatbot dialogue | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| #10 MLOps Engineer | ML operations | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |

**Resultado: 0 de 6 fallos fueron detectados por alguna auditorÃ­a.**

---

## ğŸ” AnÃ¡lisis: Â¿Por QuÃ© PasÃ³ Esto?

### 1. Sesgo hacia cÃ³digo de aplicaciÃ³n, ceguera hacia infraestructura

Las 15 auditorÃ­as se distribuyeron asÃ­:

```
CÃ³digo de AplicaciÃ³n (.cs, .tsx):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10 auditorÃ­as (67%)
IA / Machine Learning:             â–ˆâ–ˆâ–ˆâ–ˆ           4 auditorÃ­as (27%)
Infraestructura (Docker/K8s/CI):   â–‘              0 auditorÃ­as (0%)  â† BRECHA
```

**El 100% del esfuerzo de auditorÃ­a se concentrÃ³ en "Â¿el cÃ³digo estÃ¡ bien escrito?"** pero nadie preguntÃ³ **"Â¿el cÃ³digo llega correctamente a producciÃ³n?"**

### 2. AuditorÃ­as estÃ¡ticas vs verificaciÃ³n dinÃ¡mica

Todas las auditorÃ­as fueron **anÃ¡lisis estÃ¡tico de archivos**:
- Leer cÃ³digo â†’ opinar sobre patrones
- Leer configs â†’ verificar completitud
- Contar tests â†’ medir cobertura

**Ninguna auditorÃ­a ejecutÃ³ el sistema:**
- âŒ Nadie corriÃ³ `docker build` para verificar que la imagen se construye
- âŒ Nadie corriÃ³ `docker compose up` para verificar que los servicios arrancan
- âŒ Nadie desplegÃ³ en un cluster de prueba
- âŒ Nadie verificÃ³ que los health checks responden < 10s
- âŒ Nadie verificÃ³ que las queues de RabbitMQ son compatibles entre versiones

### 3. Falta de un especialista de infraestructura

| Capa del sistema | Especialista asignado | Estado |
|------------------|-----------------------|--------|
| Chatbot / IA | 4 especialistas | âœ… Ultra-cubierto |
| Frontend React | 1 especialista | âœ… Cubierto |
| Backend .NET (cÃ³digo) | 3 especialistas | âœ… Cubierto |
| Business processes | 1 especialista | âœ… Cubierto |
| **Dockerfiles** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |
| **CI/CD Workflows** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |
| **K8s Manifests** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |
| **Messaging (RabbitMQ)** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |
| **DI Wiring (Program.cs)** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |
| **Secrets & Credentials** | **Ninguno** | ğŸ”´ **NO CUBIERTO** |

### 4. La falsa sensaciÃ³n de seguridad

Con puntuaciones de **9.2/10**, **9.3/10**, **9.0/10** en mÃºltiples auditorÃ­as, se creÃ³ una percepciÃ³n de que el sistema estaba "listo para producciÃ³n". Pero estas puntuaciones solo reflejan la calidad del **cÃ³digo de aplicaciÃ³n**, no la madurez del **pipeline de deployment**.

Es como auditar que un aviÃ³n tiene excelentes motores, alas y aviÃ³nica â€” pero nadie revisÃ³ si la pista de aterrizaje existe.

---

## ğŸ¯ Los 6 Fallos Categorizados

| Fallo | CategorÃ­a | Â¿Se detecta con test unitario? | Â¿Se detecta con anÃ¡lisis estÃ¡tico? | Â¿CÃ³mo se detecta? |
|-------|-----------|:------------------------------:|:----------------------------------:|---------------------|
| F1: Health check timeout | Infra + Config | âŒ | âŒ | Smoke test en cluster real |
| F2: Registry credentials | Secrets/DevOps | âŒ | âŒ | Checklist de rotaciÃ³n de secrets |
| F3: DI mismatch | Wiring/Startup | âœ… Con integration test | âš ï¸ DifÃ­cil | `WebApplicationFactory` test |
| F4: Build cache poison | CI/CD | âŒ | âŒ | Verificar image digest post-build |
| F5: RabbitMQ args | Messaging infra | âŒ | âŒ | Queue migration strategy |
| F6: Image name mismatch | K8s â†” CI/CD | âŒ | âœ… Con linter de K8s | Cross-reference YAML validation |

**Solo 1 de 6 fallos (F3) se podrÃ­a haber detectado con tests unitarios convencionales.** Los otros 5 requieren validaciÃ³n de infraestructura que no estaba en el scope de ninguna auditorÃ­a.

---

## âœ… Recomendaciones: AuditorÃ­as Faltantes

### ğŸ”´ P0 â€” AuditorÃ­a de Infraestructura y Deployment (URGENTE)

Esta es la auditorÃ­a que faltÃ³ y habrÃ­a prevenido los 6 fallos:

| Ãrea | QuÃ© auditar |
|------|-------------|
| **Dockerfiles** | Multi-stage builds, base images, cacheo de layers, puertos, healthchecks |
| **CI/CD Workflows** | Build triggers, cache strategy, image naming, push conditions, deploy gates |
| **K8s Manifests** | Image names vs CI/CD, resource limits, probes, secrets, PVCs |
| **DI Startup** | Integration tests con `WebApplicationFactory` que validen que el DI container resuelve todos los servicios |
| **RabbitMQ Topology** | Queue arguments, exchange bindings, DLX config, migration strategy |
| **Secrets Management** | RotaciÃ³n de tokens, tipos de tokens (ephemeral vs long-lived), expiraciÃ³n |
| **Smoke Tests** | Script que haga `curl /health` a cada servicio despuÃ©s del deploy |
| **Image Consistency** | Validar que `deployments.yaml` referencia las mismas imÃ¡genes que CI/CD construye |

### ğŸŸ¡ P1 â€” ValidaciÃ³n Automatizada en CI/CD

```yaml
# Propuesta: Job de validaciÃ³n pre-deploy
validate-manifests:
  steps:
    # 1. Verificar que image names en K8s coinciden con CI/CD
    - name: Cross-reference image names
      run: |
        for svc in frontend-web gateway authservice ...; do
          K8S_IMAGE=$(grep "image:.*$svc" k8s/deployments.yaml)
          CI_IMAGE=$(grep "tags:.*$svc" .github/workflows/*.yml)
          # Compare and fail if mismatch
        done
    
    # 2. Verificar DI startup de cada servicio
    - name: DI smoke test
      run: dotnet test --filter "Category=Startup"
    
    # 3. Verificar que Dockerfiles buildean
    - name: Docker build validation
      run: docker build --target runner -t test ./backend/AuthService
```

### ğŸŸ¢ P2 â€” Startup Integration Tests

```csharp
// Test que habrÃ­a detectado Fallo #3 (IDeadLetterQueue DI crash)
[Fact]
[Trait("Category", "Startup")]
public async Task Application_Starts_Successfully()
{
    await using var app = new WebApplicationFactory<Program>();
    using var client = app.CreateClient();
    var response = await client.GetAsync("/health");
    response.EnsureSuccessStatusCode();
}
```

---

## ğŸ“ˆ LecciÃ³n Aprendida

> **"Un sistema no estÃ¡ listo para producciÃ³n cuando el cÃ³digo estÃ¡ bien escrito.
> EstÃ¡ listo cuando el cÃ³digo llega a producciÃ³n de forma confiable y repetible."**

Las auditorÃ­as existentes aseguran que el software es de calidad. Lo que faltÃ³ es asegurar que el **camino del cÃ³digo al usuario** (build â†’ push â†’ deploy â†’ run) funciona sin fricciÃ³n.

### DistribuciÃ³n ideal de auditorÃ­as para un proyecto con 86 microservicios:

```
Actual:                              Ideal:
CÃ³digo: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 67%           CÃ³digo: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 40%
IA/ML:  â–ˆâ–ˆâ–ˆâ–ˆ           27%           IA/ML:  â–ˆâ–ˆâ–ˆâ–ˆ     20%
Infra:  â–‘               0%    â†’      Infra:  â–ˆâ–ˆâ–ˆâ–ˆ     20%  â† NUEVA
DevOps:                  0%           DevOps: â–ˆâ–ˆâ–ˆ      15%  â† NUEVA
E2E:                     0%           E2E:    â–ˆ         5%  â† NUEVA
```

---

## ğŸ“Š Resumen Final

| MÃ©trica | Valor |
|---------|-------|
| Total de auditorÃ­as realizadas | 15 |
| AuditorÃ­as que cubrÃ­an infraestructura | **0** |
| Fallos en el deploy | **6** |
| Fallos prevenibles con auditorÃ­a de infra | **6 (100%)** |
| Tiempo total de resoluciÃ³n | ~6 horas |
| Costo de oportunidad | Alto (downtime en staging) |
| LecciÃ³n | Auditar cÃ³digo â‰  Auditar deployment |

---

_Post-mortem generado el 18 de febrero de 2026_  
_Proyecto OKLA â€” Deploy Staging DOKS_
