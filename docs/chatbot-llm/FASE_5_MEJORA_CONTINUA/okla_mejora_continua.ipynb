{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c845f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. SETUP Y CONFIGURACIÃ“N\n",
    "# ============================================================\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Detectar entorno\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"ğŸ–¥ï¸  Entorno: {'Google Colab' if IN_COLAB else 'Local (VS Code)'}\")\n",
    "\n",
    "# Resolver paths\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        BASE_DIR = Path('/content/drive/MyDrive/OKLA/chatbot-llm/FASE_5_MEJORA_CONTINUA')\n",
    "    except ImportError:\n",
    "        BASE_DIR = Path('.').resolve()\n",
    "else:\n",
    "    BASE_DIR = Path('.').resolve()\n",
    "    if not (BASE_DIR / 'evaluation').exists():\n",
    "        for candidate in [BASE_DIR, BASE_DIR.parent / 'FASE_5_MEJORA_CONTINUA']:\n",
    "            if (candidate / 'evaluation').exists():\n",
    "                BASE_DIR = candidate\n",
    "                break\n",
    "\n",
    "# Agregar paths para imports\n",
    "for subdir in ['evaluation', 'feedback', 'retrain', 'monitoring', 'ab_testing']:\n",
    "    path = str(BASE_DIR / subdir)\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "# â”€â”€ ConfiguraciÃ³n del LLM Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LLM_SERVER_URL = \"http://localhost:8000\"  # Cambiar si es diferente\n",
    "TEST_DATA_PATH = str(BASE_DIR.parent / 'FASE_2_DATASET' / 'output' / 'okla_test.jsonl')\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“‚ Base:       {BASE_DIR}\")\n",
    "print(f\"ğŸŒ LLM Server: {LLM_SERVER_URL}\")\n",
    "print(f\"ğŸ“Š Test data:  {TEST_DATA_PATH}\")\n",
    "print(f\"ğŸ“ Results:    {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1b. VERIFICAR CONEXIÃ“N AL LLM SERVER\n",
    "# ============================================================\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "def check_llm_server(url: str) -> dict:\n",
    "    \"\"\"Verifica que el LLM server estÃ¡ corriendo.\"\"\"\n",
    "    try:\n",
    "        req = urllib.request.Request(f\"{url}/health\")\n",
    "        with urllib.request.urlopen(req, timeout=5) as resp:\n",
    "            data = json.loads(resp.read())\n",
    "            return {'status': 'ok', 'data': data}\n",
    "    except urllib.error.URLError as e:\n",
    "        return {'status': 'error', 'error': str(e)}\n",
    "    except Exception as e:\n",
    "        return {'status': 'error', 'error': str(e)}\n",
    "\n",
    "result = check_llm_server(LLM_SERVER_URL)\n",
    "\n",
    "if result['status'] == 'ok':\n",
    "    d = result['data']\n",
    "    print(f\"âœ… LLM Server conectado\")\n",
    "    print(f\"   Status:    {d.get('status', 'N/A')}\")\n",
    "    print(f\"   Model:     {d.get('model_loaded', 'N/A')}\")\n",
    "    print(f\"   Uptime:    {d.get('uptime_seconds', 0):.0f}s\")\n",
    "    print(f\"   Requests:  {d.get('total_requests', 0):,}\")\n",
    "    SERVER_AVAILABLE = True\n",
    "else:\n",
    "    print(f\"âš ï¸  LLM Server no disponible en {LLM_SERVER_URL}\")\n",
    "    print(f\"   Error: {result['error']}\")\n",
    "    print(f\"\\n   Las secciones que requieren el server se saltarÃ¡n.\")\n",
    "    print(f\"   Para iniciar el server:\")\n",
    "    print(f\"   cd backend/ChatbotService/LlmServer && python server.py\")\n",
    "    SERVER_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2e81b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ EvaluaciÃ³n del Modelo\n",
    "\n",
    "EvalÃºa el modelo contra el test set en 5 dimensiones:\n",
    "intent accuracy, latencia, seguridad, naturalidad RD, lead capture.\n",
    "\n",
    "> âš ï¸ **Requiere:** LLM server corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. EVALUACIÃ“N DEL MODELO\n",
    "# ============================================================\n",
    "if not SERVER_AVAILABLE:\n",
    "    print(\"â­ï¸  Saltando evaluaciÃ³n â€” LLM server no disponible\")\n",
    "else:\n",
    "    from evaluate_model import (\n",
    "        evaluate_intent_accuracy,\n",
    "        evaluate_latency,\n",
    "        evaluate_safety,\n",
    "        evaluate_rd_naturalness,\n",
    "        evaluate_lead_capture,\n",
    "    )\n",
    "    \n",
    "    # Cargar test data\n",
    "    test_samples = []\n",
    "    with open(TEST_DATA_PATH) as f:\n",
    "        for line in f:\n",
    "            test_samples.append(json.loads(line))\n",
    "    print(f\"ğŸ“Š Test set cargado: {len(test_samples)} conversaciones\")\n",
    "    \n",
    "    # Limitar a N muestras para rapidez\n",
    "    MAX_EVAL_SAMPLES = 50  # Ajustar segÃºn tiempo disponible\n",
    "    eval_samples = test_samples[:MAX_EVAL_SAMPLES]\n",
    "    print(f\"   Evaluando {len(eval_samples)} muestras...\\n\")\n",
    "    \n",
    "    # â”€â”€ Intent Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"ğŸ¯ Evaluando Intent Accuracy...\")\n",
    "    intent_results = evaluate_intent_accuracy(eval_samples, LLM_SERVER_URL)\n",
    "    print(f\"   Accuracy: {intent_results['accuracy']:.1%}\")\n",
    "    print(f\"   Pass: {'âœ…' if intent_results['accuracy'] >= 0.70 else 'âŒ'}\")\n",
    "    \n",
    "    # â”€â”€ Latencia â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâš¡ Evaluando Latencia...\")\n",
    "    latency_results = evaluate_latency(eval_samples[:20], LLM_SERVER_URL)\n",
    "    print(f\"   p50: {latency_results['p50']:.0f}ms\")\n",
    "    print(f\"   p90: {latency_results['p90']:.0f}ms\")\n",
    "    print(f\"   p95: {latency_results['p95']:.0f}ms\")\n",
    "    print(f\"   Pass: {'âœ…' if latency_results['p95'] < 10000 else 'âŒ'}\")\n",
    "    \n",
    "    # â”€â”€ Safety â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ›¡ï¸ Evaluando Safety...\")\n",
    "    safety_results = evaluate_safety(LLM_SERVER_URL)\n",
    "    print(f\"   Violations: {safety_results['violations']}\")\n",
    "    print(f\"   Pass: {'âœ…' if safety_results['violations'] == 0 else 'âŒ'}\")\n",
    "    \n",
    "    # â”€â”€ Resumen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“Š RESUMEN DE EVALUACIÃ“N\")\n",
    "    print(f\"{'='*50}\")\n",
    "    all_pass = (\n",
    "        intent_results['accuracy'] >= 0.70 and\n",
    "        latency_results['p95'] < 10000 and\n",
    "        safety_results['violations'] == 0\n",
    "    )\n",
    "    print(f\"   Veredicto: {'âœ… PASS' if all_pass else 'âŒ NEEDS IMPROVEMENT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62779d8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ AnÃ¡lisis de Feedback\n",
    "\n",
    "Analiza el feedback recolectado de usuarios para identificar Ã¡reas de mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. ANÃLISIS DE FEEDBACK\n",
    "# ============================================================\n",
    "from feedback_system import FeedbackCollector, FeedbackAnalyzer\n",
    "\n",
    "FEEDBACK_DIR = BASE_DIR / 'feedback' / 'feedback_data'\n",
    "\n",
    "if FEEDBACK_DIR.exists() and any(FEEDBACK_DIR.glob('*.jsonl')):\n",
    "    collector = FeedbackCollector(str(FEEDBACK_DIR))\n",
    "    all_feedback = collector.load_all()\n",
    "    \n",
    "    print(f\"ğŸ’¬ Feedback recolectado: {len(all_feedback)} entradas\")\n",
    "    \n",
    "    if len(all_feedback) > 0:\n",
    "        # Stats bÃ¡sicos\n",
    "        stats = collector.get_stats()\n",
    "        print(f\"\\nğŸ“Š EstadÃ­sticas:\")\n",
    "        print(f\"   Promedio rating: {stats.get('avg_rating', 'N/A')}\")\n",
    "        print(f\"   ğŸ‘ Positivos: {stats.get('thumbs_up', 0)}\")\n",
    "        print(f\"   ğŸ‘ Negativos: {stats.get('thumbs_down', 0)}\")\n",
    "        \n",
    "        # AnÃ¡lisis profundo\n",
    "        analyzer = FeedbackAnalyzer(all_feedback)\n",
    "        \n",
    "        weak_intents = analyzer.identify_weak_intents()\n",
    "        if weak_intents:\n",
    "            print(f\"\\nâš ï¸ Intents dÃ©biles:\")\n",
    "            for intent, score in weak_intents[:5]:\n",
    "                print(f\"   â€¢ {intent}: {score:.2f}\")\n",
    "        \n",
    "        priorities = analyzer.generate_improvement_priorities()\n",
    "        if priorities:\n",
    "            print(f\"\\nğŸ¯ Prioridades de mejora:\")\n",
    "            for i, p in enumerate(priorities[:5], 1):\n",
    "                print(f\"   {i}. {p}\")\n",
    "else:\n",
    "    print(f\"ğŸ“­ No hay feedback recolectado aÃºn.\")\n",
    "    print(f\"   Directorio esperado: {FEEDBACK_DIR}\")\n",
    "    print(f\"\\n   El feedback se acumula cuando los usuarios interactÃºan\")\n",
    "    print(f\"   con el chatbot y califican las respuestas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcd35d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ DetecciÃ³n de Drift\n",
    "\n",
    "Compara mÃ©tricas actuales vs baseline para detectar degradaciÃ³n.\n",
    "\n",
    "> âš ï¸ **Requiere:** LLM server corriendo + baseline establecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b90635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. DETECCIÃ“N DE DRIFT\n",
    "# ============================================================\n",
    "from drift_detector import establish_baseline, detect_drift\n",
    "\n",
    "BASELINE_PATH = RESULTS_DIR / 'baseline.json'\n",
    "\n",
    "if not SERVER_AVAILABLE:\n",
    "    print(\"â­ï¸  Saltando drift detection â€” LLM server no disponible\")\n",
    "elif not BASELINE_PATH.exists():\n",
    "    print(\"ğŸ“ No hay baseline establecido. Creando baseline inicial...\")\n",
    "    print(f\"   Usando {min(30, len(test_samples))} muestras del test set...\")\n",
    "    \n",
    "    baseline = establish_baseline(\n",
    "        server_url=LLM_SERVER_URL,\n",
    "        test_samples=test_samples[:30],\n",
    "    )\n",
    "    \n",
    "    with open(BASELINE_PATH, 'w') as f:\n",
    "        json.dump(baseline, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Baseline guardado en: {BASELINE_PATH}\")\n",
    "    print(f\"   Confidence avg: {baseline.get('avg_confidence', 'N/A'):.3f}\")\n",
    "    print(f\"   Latency p95:    {baseline.get('latency_p95', 'N/A'):.0f}ms\")\n",
    "    print(f\"   Fallback rate:  {baseline.get('fallback_rate', 'N/A'):.1%}\")\n",
    "else:\n",
    "    print(\"ğŸ“ Baseline encontrado. Comparando con mÃ©tricas actuales...\")\n",
    "    \n",
    "    with open(BASELINE_PATH) as f:\n",
    "        baseline = json.load(f)\n",
    "    \n",
    "    drift_results = detect_drift(\n",
    "        server_url=LLM_SERVER_URL,\n",
    "        test_samples=test_samples[:30],\n",
    "        baseline=baseline,\n",
    "    )\n",
    "    \n",
    "    if drift_results.get('drift_detected', False):\n",
    "        print(f\"\\nğŸš¨ DRIFT DETECTADO\")\n",
    "        for signal in drift_results.get('signals', []):\n",
    "            print(f\"   âš ï¸ {signal['type']}: {signal['description']}\")\n",
    "        print(f\"\\n   RecomendaciÃ³n: Ejecutar pipeline de re-entrenamiento\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… No se detectÃ³ drift â€” Modelo estable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48f98f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ A/B Testing\n",
    "\n",
    "Gestiona experimentos A/B entre versiones del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. A/B TESTING\n",
    "# ============================================================\n",
    "from ab_testing import ABExperiment\n",
    "\n",
    "EXPERIMENTS_DIR = RESULTS_DIR / 'experiments'\n",
    "EXPERIMENTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Buscar experimentos activos\n",
    "active_experiments = list(EXPERIMENTS_DIR.glob('*.json'))\n",
    "\n",
    "if active_experiments:\n",
    "    print(f\"ğŸ”¬ Experimentos encontrados: {len(active_experiments)}\")\n",
    "    for exp_path in active_experiments:\n",
    "        with open(exp_path) as f:\n",
    "            exp_data = json.load(f)\n",
    "        \n",
    "        name = exp_data.get('name', 'unknown')\n",
    "        control = exp_data.get('control_model', 'N/A')\n",
    "        treatment = exp_data.get('treatment_model', 'N/A')\n",
    "        n_results = len(exp_data.get('results', []))\n",
    "        min_samples = exp_data.get('min_samples', 100)\n",
    "        \n",
    "        print(f\"\\n   ğŸ“‹ {name}\")\n",
    "        print(f\"      Control:   {control}\")\n",
    "        print(f\"      Treatment: {treatment}\")\n",
    "        print(f\"      Samples:   {n_results}/{min_samples}\")\n",
    "        \n",
    "        if n_results >= min_samples:\n",
    "            print(f\"      Status:    âœ… Listo para analizar\")\n",
    "        else:\n",
    "            print(f\"      Status:    â³ Recolectando ({100*n_results/min_samples:.0f}%)\")\n",
    "else:\n",
    "    print(f\"ğŸ“­ No hay experimentos A/B activos.\")\n",
    "    print(f\"\\n   Para crear uno:\")\n",
    "    print(f\"   python ab_testing/ab_testing.py create \\\\\")\n",
    "    print(f\"     --name 'v1-vs-v2' \\\\\")\n",
    "    print(f\"     --control 'okla-v1.0' \\\\\")\n",
    "    print(f\"     --treatment 'okla-v2.0' \\\\\")\n",
    "    print(f\"     --traffic-split 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baaea22",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ DecisiÃ³n de Re-entrenamiento\n",
    "\n",
    "EvalÃºa si es necesario re-entrenar el modelo basado en feedback, drift y edad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DECISIÃ“N DE RE-ENTRENAMIENTO\n",
    "# ============================================================\n",
    "from retrain_pipeline import ModelVersionManager\n",
    "\n",
    "VERSIONS_PATH = RESULTS_DIR / 'model_versions.json'\n",
    "\n",
    "manager = ModelVersionManager(str(VERSIONS_PATH))\n",
    "\n",
    "print(\"ğŸ”„ Estado del Pipeline de Re-entrenamiento\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar criterios\n",
    "should_retrain, reasons = manager.should_retrain(\n",
    "    feedback_dir=str(FEEDBACK_DIR) if FEEDBACK_DIR.exists() else None,\n",
    ")\n",
    "\n",
    "if should_retrain:\n",
    "    print(f\"\\nğŸš¨ RE-ENTRENAMIENTO RECOMENDADO\")\n",
    "    for reason in reasons:\n",
    "        print(f\"   â€¢ {reason}\")\n",
    "    print(f\"\\n   PrÃ³ximos pasos:\")\n",
    "    print(f\"   1. Ejecutar: python retrain/retrain_pipeline.py collect\")\n",
    "    print(f\"   2. Ejecutar: python retrain/retrain_pipeline.py prepare\")\n",
    "    print(f\"   3. Abrir FASE_3 notebook â†’ Select Kernel â†’ Colab â†’ GPU\")\n",
    "    print(f\"   4. Entrenar con datos actualizados\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No se requiere re-entrenamiento por ahora\")\n",
    "    if reasons:\n",
    "        for reason in reasons:\n",
    "            print(f\"   â„¹ï¸ {reason}\")\n",
    "\n",
    "# Mostrar versiones registradas\n",
    "versions = manager.list_versions()\n",
    "if versions:\n",
    "    print(f\"\\nğŸ“¦ Versiones de modelo registradas:\")\n",
    "    for v in versions:\n",
    "        status_icon = {'candidate': 'ğŸŸ¡', 'promoted': 'ğŸŸ¢', 'retired': 'âšª'}\n",
    "        icon = status_icon.get(v.get('status', ''), 'â“')\n",
    "        print(f\"   {icon} {v['version']} â€” {v['status']} ({v.get('date', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec22182",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ Monitoreo Prometheus\n",
    "\n",
    "Estado de las mÃ©tricas Prometheus del LLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. VERIFICAR MÃ‰TRICAS PROMETHEUS\n",
    "# ============================================================\n",
    "\n",
    "if not SERVER_AVAILABLE:\n",
    "    print(\"â­ï¸  Saltando Prometheus â€” LLM server no disponible\")\n",
    "else:\n",
    "    try:\n",
    "        req = urllib.request.Request(f\"{LLM_SERVER_URL}/metrics\")\n",
    "        with urllib.request.urlopen(req, timeout=5) as resp:\n",
    "            metrics_text = resp.read().decode('utf-8')\n",
    "        \n",
    "        print(\"ğŸ“¡ MÃ©tricas Prometheus disponibles en /metrics\")\n",
    "        print(f\"   URL: {LLM_SERVER_URL}/metrics\")\n",
    "        print()\n",
    "        \n",
    "        # Parsear mÃ©tricas clave\n",
    "        key_metrics = [\n",
    "            'okla_llm_requests_total',\n",
    "            'okla_llm_requests_success_total',\n",
    "            'okla_llm_avg_response_time_ms',\n",
    "            'okla_llm_uptime_seconds',\n",
    "            'okla_llm_model_loaded',\n",
    "        ]\n",
    "        \n",
    "        print(\"   MÃ©tricas clave:\")\n",
    "        for line in metrics_text.split('\\n'):\n",
    "            for metric in key_metrics:\n",
    "                if line.startswith(metric) and not line.startswith('#'):\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        name = parts[0].split('{')[0]\n",
    "                        value = parts[-1]\n",
    "                        print(f\"   â€¢ {name}: {value}\")\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š Dashboard Grafana:\")\n",
    "        print(f\"   Importar: monitoring/grafana-dashboard.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Endpoint /metrics no disponible: {e}\")\n",
    "        print(f\"   Â¿El server tiene prometheus-client instalado?\")\n",
    "        print(f\"   pip install prometheus-client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d22bd",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“‹ Resumen y Acciones\n",
    "\n",
    "### Comandos CLI Alternativos\n",
    "\n",
    "Todos los mÃ³dulos tambiÃ©n funcionan como scripts CLI:\n",
    "\n",
    "```bash\n",
    "# EvaluaciÃ³n completa\n",
    "python evaluation/evaluate_model.py --test-data ../FASE_2_DATASET/output/okla_test.jsonl --server-url http://localhost:8000\n",
    "\n",
    "# AnÃ¡lisis de feedback\n",
    "python feedback/feedback_system.py report --data-dir ./feedback/feedback_data\n",
    "\n",
    "# Drift detection\n",
    "python monitoring/drift_detector.py monitor --server-url http://localhost:8000 --baseline results/baseline.json\n",
    "\n",
    "# A/B testing\n",
    "python ab_testing/ab_testing.py analyze --experiment results/experiments/exp.json\n",
    "\n",
    "# Retrain check\n",
    "python retrain/retrain_pipeline.py check --versions-file results/model_versions.json\n",
    "```\n",
    "\n",
    "### ğŸ”Œ Workflow VS Code + Colab\n",
    "\n",
    "| Tarea | DÃ³nde ejecutar |\n",
    "|-------|---------------|\n",
    "| EvaluaciÃ³n rÃ¡pida (< 50 samples) | **Local** |\n",
    "| EvaluaciÃ³n completa (500+ samples) | **Colab** (mÃ¡s RAM/CPU) |\n",
    "| Feedback analysis | **Local** |\n",
    "| Drift detection | **Local** |\n",
    "| Re-entrenamiento | **Colab con GPU** (FASE 3 notebook) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia 1.11",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
