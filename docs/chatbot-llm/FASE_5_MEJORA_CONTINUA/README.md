# ğŸ“ˆ FASE 5 â€” Mejora Continua del Chatbot LLM OKLA

> **Estado:** âœ… Completa  
> **Fecha:** Febrero 2026  
> **Prerequisito:** FASE 4 (Deployment) completada â€” Dialogflow **ELIMINADO**, LLM en producciÃ³n.

---

## ğŸ“‹ Resumen

FASE 5 implementa el ecosistema completo de **mejora continua** para el chatbot LLM OKLA. Incluye 6 mÃ³dulos que trabajan en conjunto para monitorear, evaluar, recolectar feedback, detectar degradaciÃ³n, comparar modelos y automatizar el re-entrenamiento.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CICLO DE MEJORA CONTINUA                  â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ Monitor  â”‚â”€â”€â”€â–¶â”‚  Evaluar  â”‚â”€â”€â”€â–¶â”‚ Recolectar       â”‚    â”‚
â”‚   â”‚ (Prom +  â”‚    â”‚ (evaluate â”‚    â”‚ Feedback          â”‚    â”‚
â”‚   â”‚  Grafana)â”‚    â”‚  _model)  â”‚    â”‚ (feedback_system) â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â–²                                     â”‚              â”‚
â”‚        â”‚                                     â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ A/B Test â”‚â—€â”€â”€â”€â”‚ Re-train  â”‚â—€â”€â”€â”€â”‚ Detectar Drift   â”‚    â”‚
â”‚   â”‚ (ab_test â”‚    â”‚ (retrain  â”‚    â”‚ (drift_detector)  â”‚    â”‚
â”‚   â”‚  ing)    â”‚    â”‚  pipeline)â”‚    â”‚                   â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Estructura de Archivos

```
FASE_5_MEJORA_CONTINUA/
â”œâ”€â”€ README.md                          â† Este archivo
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate_model.py             â† Pipeline de evaluaciÃ³n automatizada
â”œâ”€â”€ feedback/
â”‚   â””â”€â”€ feedback_system.py            â† RecolecciÃ³n y anÃ¡lisis de feedback
â”œâ”€â”€ retrain/
â”‚   â””â”€â”€ retrain_pipeline.py           â† Pipeline de re-entrenamiento
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ drift_detector.py             â† DetecciÃ³n de degradaciÃ³n del modelo
â”‚   â”œâ”€â”€ prometheus_metrics.py         â† Exportador de mÃ©tricas Prometheus
â”‚   â””â”€â”€ grafana-dashboard.json        â† Dashboard Grafana importable
â””â”€â”€ ab_testing/
    â””â”€â”€ ab_testing.py                 â† Framework de pruebas A/B
```

**Archivos modificados en producciÃ³n:**

| Archivo                                             | Cambio                                              |
| --------------------------------------------------- | --------------------------------------------------- |
| `backend/ChatbotService/LlmServer/server.py`        | + endpoint `/metrics`, + instrumentaciÃ³n Prometheus |
| `backend/ChatbotService/LlmServer/requirements.txt` | + `prometheus-client>=0.21.0`                       |

---

## ğŸ”§ MÃ³dulos

### 1. ğŸ“Š EvaluaciÃ³n Automatizada (`evaluation/evaluate_model.py`)

EvalÃºa la calidad del modelo contra un test set con 5 dimensiones:

| DimensiÃ³n           | MÃ©trica                       | Threshold Pass |
| ------------------- | ----------------------------- | -------------- |
| **Intent Accuracy** | Match rate vs expected intent | â‰¥ 70%          |
| **Latencia**        | p50, p90, p95, p99            | p95 < 10s      |
| **Seguridad**       | 10 prompts adversarios        | 0 violaciones  |
| **Naturalidad RD**  | Vocabulario dominicano        | â‰¥ 50% score    |
| **Lead Capture**    | Captura de datos de contacto  | â‰¥ 40% rate     |

#### Uso

```bash
# Evaluar contra test set
python evaluate_model.py \
    --test-data ../FASE_2_DATASET/output/test.jsonl \
    --server-url http://localhost:8000 \
    --output-dir ./results

# Solo latencia
python evaluate_model.py \
    --test-data test.jsonl \
    --server-url http://localhost:8000 \
    --skip-safety --skip-naturalness --skip-lead
```

#### Output

- `results/evaluation_report_YYYYMMDD_HHMMSS.json` â€” Datos crudos
- `results/evaluation_report_YYYYMMDD_HHMMSS.md` â€” Reporte legible

---

### 2. ğŸ’¬ Sistema de Feedback (`feedback/feedback_system.py`)

Recolecta, analiza y exporta feedback de usuarios para alimentar el re-entrenamiento.

#### CategorÃ­as de Feedback (12)

| CategorÃ­a            | DescripciÃ³n                 |
| -------------------- | --------------------------- |
| `wrong_intent`       | Intent incorrecto detectado |
| `hallucination`      | InformaciÃ³n inventada       |
| `incomplete`         | Respuesta incompleta        |
| `wrong_language`     | No usa espaÃ±ol dominicano   |
| `too_formal`         | Tono demasiado formal       |
| `too_informal`       | Tono demasiado informal     |
| `privacy_leak`       | FiltrÃ³ datos sensibles      |
| `wrong_price`        | Precio incorrecto           |
| `wrong_vehicle_info` | Info de vehÃ­culo incorrecta |
| `slow_response`      | Respuesta muy lenta         |
| `good_response`      | Respuesta buena (positivo)  |
| `other`              | Otro                        |

#### Uso

```bash
# Analizar todo el feedback recolectado
python feedback_system.py analyze --data-dir ./feedback_data

# Exportar ejemplos para re-entrenamiento
python feedback_system.py export \
    --data-dir ./feedback_data \
    --output-dir ./export \
    --min-rating 4

# Generar reporte completo
python feedback_system.py report --data-dir ./feedback_data
```

#### Almacenamiento

- Formato: JSONL con rotaciÃ³n diaria
- Archivos: `feedback_data/feedback_YYYYMMDD.jsonl`
- Cada lÃ­nea: `{ session_id, timestamp, rating, thumbs, user_query, bot_response, correction, category, metadata }`

---

### 3. ğŸ”„ Pipeline de Re-entrenamiento (`retrain/retrain_pipeline.py`)

Automatiza el ciclo completo: recolecciÃ³n â†’ validaciÃ³n â†’ merge â†’ split â†’ script de Colab.

#### Flujo

```
Feedback (JSONL)  â”€â”€â”
                    â”œâ”€â”€â–¶ Collect â”€â”€â–¶ Deduplicate â”€â”€â–¶ Validate
Conversations (DB) â”€â”˜                                  â”‚
                                                        â–¼
                                             Merge con Dataset Original
                                             (ratio configurable 70/30)
                                                        â”‚
                                                        â–¼
                                             Split (85% train / 10% eval / 5% test)
                                                        â”‚
                                                        â–¼
                                             Generate Colab Script
                                             (Unsloth + QLoRA + LoRA merge)
```

#### Modelo de Versiones

| Estado      | DescripciÃ³n                                       |
| ----------- | ------------------------------------------------- |
| `candidate` | Modelo reciÃ©n entrenado, en evaluaciÃ³n            |
| `promoted`  | Modelo que pasÃ³ evaluaciÃ³n, listo para producciÃ³n |
| `retired`   | Modelo anterior, reemplazado por versiÃ³n nueva    |

#### Criterios de Re-entrenamiento AutomÃ¡tico

- **Feedback count â‰¥ 50** nuevos ejemplos
- **Edad del modelo â‰¥ 30 dÃ­as**
- **Accuracy < 70%** en Ãºltima evaluaciÃ³n

#### Uso

```bash
# Recolectar nuevos datos de feedback
python retrain_pipeline.py collect \
    --feedback-dir ../feedback/feedback_data \
    --output-dir ./collected

# Preparar dataset combinado
python retrain_pipeline.py prepare \
    --new-data ./collected/combined.jsonl \
    --original-data ../../FASE_2_DATASET/output/train.jsonl \
    --output-dir ./prepared \
    --original-ratio 0.7

# Verificar si toca re-entrenar
python retrain_pipeline.py check \
    --versions-file ./model_versions.json \
    --feedback-dir ../feedback/feedback_data

# Generar script de Colab
python retrain_pipeline.py generate-script \
    --train-data ./prepared/train.jsonl \
    --eval-data ./prepared/eval.jsonl \
    --output-script ./retrain_colab.py
```

---

### 4. ğŸ“‰ Detector de Drift (`monitoring/drift_detector.py`)

Detecta degradaciÃ³n del modelo en producciÃ³n comparando mÃ©tricas actuales con un baseline.

#### SeÃ±ales de Drift (5)

| SeÃ±al                       | Threshold | DescripciÃ³n                    |
| --------------------------- | --------- | ------------------------------ |
| `confidence_drop`           | -10%      | Confianza promedio cayÃ³        |
| `fallback_rate_increase`    | +5pp      | MÃ¡s respuestas de fallback     |
| `latency_increase`          | +50%      | Latencia p95 aumentÃ³           |
| `intent_distribution_shift` | KL > 0.3  | DistribuciÃ³n de intents cambiÃ³ |
| `token_usage_change`        | Â±30%      | Tokens promedio cambiÃ³         |

#### Alertas

- **Slack webhook** automÃ¡tico cuando se detecta drift
- Payload incluye: seÃ±ales detectadas, mÃ©tricas actual vs baseline, recomendaciones

#### Uso

```bash
# Establecer baseline (ejecutar cuando el modelo es nuevo)
python drift_detector.py baseline \
    --server-url http://localhost:8000 \
    --test-data ../FASE_2_DATASET/output/test.jsonl \
    --output ./baseline.json

# Monitorear vs baseline
python drift_detector.py monitor \
    --server-url http://localhost:8000 \
    --test-data ../FASE_2_DATASET/output/test.jsonl \
    --baseline ./baseline.json \
    --slack-webhook https://hooks.slack.com/services/XXX/YYY/ZZZ

# Comparar dos baselines
python drift_detector.py compare \
    --baseline-a ./baseline_v1.json \
    --baseline-b ./baseline_v2.json
```

---

### 5. ğŸ”€ Framework de A/B Testing (`ab_testing/ab_testing.py`)

Compara versiones de modelo en producciÃ³n con significancia estadÃ­stica.

#### Tests EstadÃ­sticos

| Test               | Tipo de MÃ©trica    | Uso                             |
| ------------------ | ------------------ | ------------------------------- |
| **Chi-squared**    | Proporciones (0/1) | Satisfaction rate, lead capture |
| **Welch's t-test** | Medias continuas   | Latency, confidence             |

#### Sistema de PuntuaciÃ³n para DecisiÃ³n

| MÃ©trica      | Peso  | JustificaciÃ³n                  |
| ------------ | ----- | ------------------------------ |
| SatisfacciÃ³n | 3 pts | Experiencia de usuario primero |
| Lead capture | 2 pts | Objetivo de negocio            |
| Latencia     | 1 pt  | Performance tÃ©cnica            |
| Confianza    | 1 pt  | Calidad del modelo             |

#### Uso

```bash
# Crear experimento
python ab_testing.py create \
    --name "v1-vs-v2" \
    --control "okla-v1.0" \
    --treatment "okla-v2.0" \
    --traffic-split 50 \
    --min-samples 100 \
    --output-dir ./experiments

# Registrar resultado de una interacciÃ³n
python ab_testing.py log \
    --experiment ./experiments/v1-vs-v2.json \
    --variant control \
    --satisfied true \
    --lead-captured false \
    --latency-ms 1250 \
    --confidence 0.87

# Analizar resultados
python ab_testing.py analyze \
    --experiment ./experiments/v1-vs-v2.json

# Decidir ganador
python ab_testing.py decide \
    --experiment ./experiments/v1-vs-v2.json
```

---

### 6. ğŸ“¡ MÃ©tricas Prometheus + Dashboard Grafana

#### MÃ©tricas Expuestas en `/metrics`

| MÃ©trica                            | Tipo      | DescripciÃ³n                  |
| ---------------------------------- | --------- | ---------------------------- |
| `okla_llm_requests_total`          | Counter   | Total de requests            |
| `okla_llm_requests_success_total`  | Counter   | Requests exitosas            |
| `okla_llm_requests_error_total`    | Counter   | Requests fallidas (por tipo) |
| `okla_llm_response_duration_ms`    | Histogram | Latencia (11 buckets)        |
| `okla_llm_tokens_total`            | Counter   | Tokens totales               |
| `okla_llm_prompt_tokens_total`     | Counter   | Tokens de prompt             |
| `okla_llm_completion_tokens_total` | Counter   | Tokens de completion         |
| `okla_llm_model_loaded`            | Gauge     | Modelo cargado (1/0)         |
| `okla_llm_uptime_seconds`          | Gauge     | Uptime del server            |
| `okla_llm_active_requests`         | Gauge     | Requests en vuelo            |
| `okla_llm_avg_response_time_ms`    | Gauge     | Promedio rolling de latencia |
| `okla_llm_model_info`              | Info      | Metadatos del modelo         |

#### ConfiguraciÃ³n de Prometheus

Agregar a `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: "llm-server"
    scrape_interval: 15s
    metrics_path: /metrics
    static_configs:
      - targets: ["llm-server:8000"]
        labels:
          service: "okla-chatbot-llm"
          environment: "production"
```

#### Dashboard Grafana

1. Importar `monitoring/grafana-dashboard.json` en Grafana
2. Configurar datasource Prometheus
3. Dashboard incluye 4 secciones:
   - **Estado General** â€” Model status, uptime, request count, avg latency, success rate
   - **Latencia e Inferencia** â€” p50/p90/p95/p99, request rate, tokens/request, throughput
   - **Intents y Calidad** â€” DistribuciÃ³n de intents (pie chart), confianza (percentiles)
   - **Recursos del Sistema** â€” Memory RSS/VMS, CPU usage

---

## ğŸ”„ Flujo Operativo Recomendado

### Semanal

1. **Revisar Dashboard Grafana** â€” latencia, error rate, distribuciÃ³n de intents
2. **Ejecutar evaluaciÃ³n** â€” `python evaluate_model.py --test-data test.jsonl`
3. **Analizar feedback** â€” `python feedback_system.py report`

### Mensual

4. **Detectar drift** â€” `python drift_detector.py monitor --baseline baseline.json`
5. **Verificar re-entrenamiento** â€” `python retrain_pipeline.py check`
6. Si aplica: recolectar datos â†’ preparar â†’ generar script â†’ entrenar en Colab

### Por Release de Modelo

7. **Establecer nuevo baseline** â€” `python drift_detector.py baseline`
8. **Crear experimento A/B** â€” `python ab_testing.py create`
9. Recolectar ~100+ interacciones por variante
10. **Decidir ganador** â€” `python ab_testing.py decide`
11. Promover modelo ganador â€” `python retrain_pipeline.py` (model versions)

---

## ğŸ› ï¸ Dependencias

### Python (ya incluidas en requirements.txt del LLM server)

```
prometheus-client>=0.21.0   # MÃ©tricas Prometheus
```

### MÃ³dulos de FASE 5 (standalone, sin deps adicionales)

Todos los scripts usan Ãºnicamente stdlib de Python 3.11+ (`json`, `statistics`, `math`, `hashlib`, `datetime`, `re`, `pathlib`, `argparse`, `http.client`). La Ãºnica dependencia externa es `requests` para comunicaciÃ³n HTTP con el LLM server.

```bash
pip install requests  # Para evaluate_model.py y drift_detector.py
```

### Infraestructura

| Componente           | Para quÃ©                           |
| -------------------- | ---------------------------------- |
| **Prometheus**       | Scraping de mÃ©tricas de `/metrics` |
| **Grafana**          | VisualizaciÃ³n del dashboard        |
| **Slack** (opcional) | Alertas de drift                   |

---

## ğŸ“Š RelaciÃ³n entre Fases

```
FASE 1 (Prompts)
    â””â”€â”€â–¶ FASE 2 (Dataset) â”€â”€â–¶ 2,989 conversaciones
            â””â”€â”€â–¶ FASE 3 (Training) â”€â”€â–¶ Modelo GGUF Q4_K_M
                    â””â”€â”€â–¶ FASE 4 (Deploy) â”€â”€â–¶ LLM Server en K8s
                            â””â”€â”€â–¶ FASE 5 (Mejora Continua) â—€â”€â”€ EstÃ¡s aquÃ­
                                    â”‚
                                    â”œâ”€â”€ Monitoreo en tiempo real (Prometheus + Grafana)
                                    â”œâ”€â”€ EvaluaciÃ³n periÃ³dica (test set + safety + RD)
                                    â”œâ”€â”€ Feedback â†’ anÃ¡lisis â†’ exportar para retrain
                                    â”œâ”€â”€ Drift detection â†’ alertas â†’ trigger retrain
                                    â”œâ”€â”€ Re-entrenamiento â†’ Colab â†’ nuevo GGUF
                                    â””â”€â”€ A/B testing â†’ significancia estadÃ­stica â†’ promote
```

---

## âš ï¸ Notas Importantes

1. **Dialogflow estÃ¡ ELIMINADO** â€” Todo el stack de NLU es ahora LLM (Llama 3 8B fine-tuned)
2. **Prometheus es nuevo** â€” Antes de FASE 5, el monitoring era solo in-memory counters y DB queries
3. **Los scripts de evaluaciÃ³n requieren el LLM server corriendo** â€” Usan HTTP contra `/v1/chat/completions`
4. **El re-entrenamiento es manual via Colab** â€” El script se genera automÃ¡ticamente pero debe ejecutarse en Google Colab con GPU
5. **A/B testing requiere routing** â€” El `LlmService.cs` en ChatbotService debe implementar lÃ³gica de routing de trÃ¡fico entre variantes

---

_FASE 5 â€” Mejora Continua â€” OKLA Chatbot LLM â€” Febrero 2026_
