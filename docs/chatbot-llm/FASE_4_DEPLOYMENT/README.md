# FASE 4 â€” Deployment: Dialogflow â†’ LLM (COMPLETADO)

## ğŸ¯ Objetivo

**ELIMINAR Google Dialogflow ES** y reemplazarlo con un modelo **Llama 3 8B fine-tuned** servido via **llama.cpp**.

## ğŸ“‹ Resumen de Cambios

### âœ… ELIMINADO â€” Google Dialogflow ES

- âŒ `Google.Cloud.Dialogflow.V2` NuGet package
- âŒ `Google.Apis.Auth` NuGet package
- âŒ `DialogflowService.cs` â€” Servicio que llamaba a Dialogflow API
- âŒ `IDialogflowService` â€” Interface con 7 mÃ©todos Dialogflow
- âŒ `DialogflowDetectionResult` â€” Modelo de respuesta Dialogflow
- âŒ `DialogflowAgentInfo` â€” Info del agente Dialogflow
- âŒ `DialogflowHealthStatus` â€” Health del servicio Dialogflow
- âŒ `DialogflowSettings` â€” ConfiguraciÃ³n (ProjectId, Credentials, etc.)
- âŒ SecciÃ³n `"Dialogflow"` en `appsettings.json`

### âœ… CREADO â€” LLM Inference Server

- âœ… `LlmServer/server.py` â€” Servidor FastAPI compatible con OpenAI API
- âœ… `LlmServer/Dockerfile` â€” Container para llama.cpp
- âœ… `LlmServer/requirements.txt` â€” Dependencias Python

### âœ… CREADO â€” Servicios C# Nuevos

- âœ… `ILlmService` â€” Interface con 5 mÃ©todos LLM
- âœ… `LlmService.cs` â€” ImplementaciÃ³n con HTTP client + Polly resilience
- âœ… `LlmSettings` â€” ConfiguraciÃ³n (ServerUrl, ModelId, Temperature, etc.)
- âœ… `LlmDetectionResult` â€” Modelo de respuesta con campos adicionales
- âœ… `LlmModelInfo` â€” InformaciÃ³n del modelo GGUF
- âœ… `LlmHealthStatus` â€” Health con mÃ©tricas detalladas
- âœ… `LlmLeadSignals` â€” SeÃ±ales de lead del modelo
- âœ… `LlmChatMessage` â€” Mensajes para contexto

### âœ… MODIFICADO â€” Servicios Existentes

| Archivo                         | Cambio                                                                                 |
| ------------------------------- | -------------------------------------------------------------------------------------- |
| `IServices.cs`                  | `IDialogflowService` â†’ `ILlmService` (5 mÃ©todos)                                       |
| `ServiceModels.cs`              | Todas las clases `Dialogflow*` â†’ `Llm*`                                                |
| `DependencyInjection.cs`        | DI: `ILlmService` â†’ `LlmService` + `LlmServer` HttpClient                              |
| `SessionCommandHandlers.cs`     | Pipeline: `_dialogflowService.DetectIntentAsync` â†’ `_llmService.GenerateResponseAsync` |
| `AutoLearningService.cs`        | `IDialogflowService` â†’ `ILlmService`, Dialogflow API calls â†’ training data storage     |
| `HealthMonitoringService.cs`    | `CheckDialogflowHealthAsync` â†’ `CheckLlmHealthAsync`                                   |
| `MaintenanceCommandHandlers.cs` | `IDialogflowService` â†’ `ILlmService`                                                   |
| `MaintenanceCommands.cs`        | `DialogflowProjectId` â†’ `LlmServerUrl`, `BypassDialogflow` â†’ `BypassLlm`               |
| `IRepositories.cs`              | + `GetRecentBySessionTokenAsync`, + `GetLlmCallsCountAsync`                            |
| `Repositories.cs`               | ImplementaciÃ³n de nuevos mÃ©todos del repositorio                                       |
| `appsettings.json`              | SecciÃ³n `"Dialogflow"` â†’ `"LlmService"`                                                |
| `Infrastructure.csproj`         | Removidos NuGet packages de Google Dialogflow                                          |

### âœ… CREADO â€” Infraestructura

- âœ… `docker-compose.llm.yml` â€” Docker Compose para desarrollo local
- âœ… `k8s/llm-server.yaml` â€” Kubernetes manifests (Deployment, Service, PVC, ConfigMap, Job)

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP POST     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChatbotServiceâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   LLM Server     â”‚
â”‚   (.NET 8)      â”‚  /v1/chat/        â”‚   (llama.cpp)    â”‚
â”‚                 â”‚  completions      â”‚                   â”‚
â”‚   LlmService.cs â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   server.py       â”‚
â”‚   (HttpClient   â”‚     JSON          â”‚   FastAPI          â”‚
â”‚   + Polly)      â”‚                   â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                      â”‚ GGUF Model  â”‚
                                      â”‚ Q4_K_M      â”‚
                                      â”‚ ~4.5 GB     â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline de Mensajes (ANTES vs DESPUÃ‰S)

**ANTES (Dialogflow):**

```
Usuario â†’ QuickResponse check â†’ Dialogflow ES API â†’ Parse intent â†’ Save â†’ Response
```

**DESPUÃ‰S (LLM):**

```
Usuario â†’ QuickResponse check â†’ LLM Server (llama.cpp) â†’ Parse JSON â†’ Save â†’ Response
```

### Formato de Respuesta del LLM

El modelo estÃ¡ fine-tuned para responder en JSON:

```json
{
  "response": "Â¡Claro! Tenemos un Toyota Corolla 2023...",
  "intent": "vehicle_inquiry",
  "confidence": 0.95,
  "parameters": {
    "make": "Toyota",
    "model": "Corolla",
    "year": "2023"
  },
  "leadSignals": {
    "purchaseIntent": 0.8,
    "urgency": 0.6,
    "preferredContact": "whatsapp",
    "shouldTransferToAgent": false
  }
}
```

---

## ğŸš€ Deployment

### Desarrollo Local

```bash
# 1. Descargar el modelo GGUF (~4.5 GB)
mkdir -p backend/ChatbotService/LlmServer/models
# Copiar okla-llama3-8b-q4_k_m.gguf al directorio models/

# 2. Levantar con Docker Compose
cd backend/ChatbotService
docker compose -f docker-compose.llm.yml up -d

# 3. Verificar salud del servidor LLM
curl http://localhost:8000/health
# â†’ {"status": "healthy", "model_loaded": true, ...}

# 4. Probar inferencia
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "Eres OKLA Assistant."},
      {"role": "user", "content": "Â¿Tienen Toyota Corolla disponible?"}
    ]
  }'
```

### Kubernetes (DOKS)

```bash
# 1. Descargar modelo al PersistentVolume
kubectl apply -f k8s/llm-server.yaml
kubectl wait --for=condition=complete job/download-llm-model -n okla --timeout=600s

# 2. Verificar deployment
kubectl get pods -n okla -l app=llm-server
kubectl logs -f deployment/llm-server -n okla

# 3. Health check interno
kubectl exec -it deployment/chatbotservice -n okla -- \
  curl http://llm-server:8000/health
```

---

## ğŸ“Š ComparaciÃ³n: Dialogflow vs LLM

| Aspecto             | Dialogflow ES            | LLM (Llama 3 8B)                 |
| ------------------- | ------------------------ | -------------------------------- |
| **Costo**           | $0.002/request + escala  | Fijo (~$50/mes servidor)         |
| **Latencia**        | ~200-500ms               | ~500-2000ms (CPU)                |
| **Idioma**          | Multi (pero genÃ©rico)    | Dominican Spanish (fine-tuned)   |
| **Contexto**        | Limitado a contexts      | Full conversation history        |
| **Intents**         | Predefinidos manualmente | Detectados automÃ¡ticamente       |
| **PersonalizaciÃ³n** | Limitada                 | Total (re-entrenamiento)         |
| **Vendor lock-in**  | Google Cloud             | Self-hosted                      |
| **Compliance**      | Datos en Google          | Datos en nuestros servidores     |
| **Lead scoring**    | No nativo                | Integrado en modelo              |
| **Legal RD**        | No conoce leyes RD       | Entrenado con Ley 358-05, 172-13 |

---

## âš ï¸ Notas Importantes

1. **Backward Compatibility DB:** Los campos `DialogflowIntentName` en la entidad `ChatMessage` mantienen el nombre de columna en la base de datos para no requerir migraciÃ³n destructiva. El campo almacena ahora el intent detectado por el LLM.

2. **Auto-Learning:** Las funciones `CreateIntentAsync`, `AddTrainingPhrasesAsync`, y `TrainAgentAsync` que llamaban a Dialogflow API ahora almacenan las sugerencias para futuros ciclos de re-entrenamiento del modelo LLM.

3. **Modelo GGUF:** El archivo `okla-llama3-8b-q4_k_m.gguf` (~4.5 GB) debe descargarse desde HuggingFace Hub y montarse como volumen. No se incluye en la imagen Docker.

4. **GPU Support:** Por defecto `N_GPU_LAYERS=0` (CPU only). Para GPU, configurar a `-1` (all layers) y usar una imagen Docker con CUDA support.

---

_FASE 4 completada â€” Google Dialogflow ES ELIMINADO del sistema_
