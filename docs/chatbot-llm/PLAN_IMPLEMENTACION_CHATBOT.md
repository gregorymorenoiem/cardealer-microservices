# ğŸ¤– Plan de ImplementaciÃ³n â€” Chatbot OKLA con Llama 3

**Fecha:** Febrero 15, 2026
**Objetivo:** Implementar un chatbot IA de ventas vehiculares usando Llama 3 8B fine-tuned, desplegado en DOKS, integrado en la plataforma OKLA.

---

## ğŸ“Š Estado Actual (AuditorÃ­a)

| Componente                          |   Estado   | Detalle                                                                  |
| ----------------------------------- | :--------: | ------------------------------------------------------------------------ |
| **FASE 1** â€” 10 Prompts modulares   |  âœ… 100%   | 10 archivos .md con prompts, ejemplos, cÃ³digo .NET                       |
| **FASE 2** â€” Dataset sintÃ©tico      |  âœ… 100%   | 2,989 conversaciones JSONL generadas (train/eval/test)                   |
| **FASE 3** â€” Notebook fine-tuning   |  âœ… 100%   | 32 celdas listas para Colab (QLoRA, Llama 3 8B)                          |
| **FASE 4** â€” Backend ChatbotService |  âœ… ~90%   | Clean Architecture completa, `LlmService.cs`, `server.py`, K8s manifests |
| **FASE 4** â€” Gateway routes         |   âŒ 0%    | No hay rutas chatbot en `ocelot.*.json`                                  |
| **FASE 4** â€” Docker Compose         |   âŒ 0%    | No hay entry en `docker-compose.yml` principal                           |
| **FASE 5** â€” MLOps scripts          |  âœ… 100%   | evaluation, feedback, drift, monitoring, A/B testing                     |
| **Frontend** â€” Widget chatbot       |   âŒ 0%    | No existe componente en `web-next`                                       |
| **Frontend** â€” Servicio API chat    | ğŸŸ¡ Parcial | Existe en `_DESCARTADOS` (489 lÃ­neas, Vite) â€” necesita port a Next.js    |
| **Modelo entrenado** â€” GGUF         |   âŒ 0%    | Necesita ejecuciÃ³n en GPU (Colab)                                        |

---

## ğŸš€ Plan por Pasos (10 Pasos)

### PASO 1: Entrenar el Modelo en Google Colab â±ï¸ ~3 horas

**QuÃ© hacer:**

1. Abrir `FASE_3_TRAINING/okla_finetune_llama3.ipynb` en VS Code
2. Conectar a Google Colab (extensiÃ³n VS Code Colab o directamente en colab.google.com)
3. Subir dataset a Google Drive:
   ```
   Drive/OKLA/dataset/
   â”œâ”€â”€ okla_train.jsonl  (13.4 MB)
   â”œâ”€â”€ okla_eval.jsonl   (1.68 MB)
   â””â”€â”€ okla_test.jsonl   (1.66 MB)
   ```
4. Ejecutar las 21 celdas de cÃ³digo secuencialmente
5. Verificar mÃ©tricas de calidad:
   - Eval loss < 1.5 (ideal < 0.8)
   - Perplexity < 5.0 (ideal < 2.5)
   - JSON vÃ¡lido > 85%
   - Intent correcto > 70%
6. Descargar el modelo GGUF Q4_K_M (~4.7 GB) desde Drive

**Output:** `okla-llama3-8b-chatbot.Q4_K_M.gguf` (~4.7 GB)

**DÃ³nde colocarlo:** `backend/ChatbotService/LlmServer/models/`

> âš ï¸ **Alternativa sin GPU:** Si no tienes acceso a Colab con GPU T4/A100, puedes:
>
> - Usar RunPod.io (~$0.50/hora con A100)
> - Usar Lambda Labs (~$1.10/hora con A10)
> - Usar Vast.ai (~$0.30/hora con RTX 4090)

---

### PASO 2: Probar el LLM Server Localmente â±ï¸ ~30 min

**QuÃ© hacer:**

1. Colocar el modelo GGUF en `backend/ChatbotService/LlmServer/models/`
2. Ejecutar el LLM server localmente:
   ```bash
   cd backend/ChatbotService
   docker compose -f docker-compose.llm.yml up llm-server
   ```
3. Probar el endpoint:
   ```bash
   curl -X POST http://localhost:8000/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
       "messages": [
         {"role": "system", "content": "Eres OKLA Bot, asistente de ventas vehiculares en RD."},
         {"role": "user", "content": "Klk, busco una yipeta pa la familia"}
       ],
       "temperature": 0.7,
       "max_tokens": 512
     }'
   ```
4. Verificar que responde JSON con `intent`, `confidence`, `response`, `leadSignals`
5. Verificar health: `curl http://localhost:8000/health`

**Criterios de Ã©xito:**

- âœ… Server arranca en < 2 minutos
- âœ… Responde en < 5 segundos
- âœ… JSON vÃ¡lido con todos los campos
- âœ… Entiende slang dominicano (yipeta, guagua, pela'o)

---

### PASO 3: Agregar ChatbotService al Docker Compose Principal â±ï¸ ~15 min

**QuÃ© hacer:**

1. Agregar al `docker-compose.yml` principal:

   ```yaml
   chatbotservice:
     build:
       context: ./backend/ChatbotService
       dockerfile: Dockerfile
     ports:
       - "5060:8080"
     environment:
       - ASPNETCORE_ENVIRONMENT=Docker
       - ConnectionStrings__DefaultConnection=Host=postgres;Database=chatbot_db;Username=postgres;Password=postgres123
       - LlmService__ServerUrl=http://llm-server:8000
       - RabbitMq__HostName=rabbitmq
       - Redis__ConnectionString=redis:6379
     depends_on:
       postgres:
         condition: service_healthy
       llm-server:
         condition: service_healthy
     networks:
       - cardealer-network

   llm-server:
     build:
       context: ./backend/ChatbotService/LlmServer
       dockerfile: Dockerfile
     ports:
       - "8000:8000"
     volumes:
       - ./backend/ChatbotService/LlmServer/models:/app/models
     environment:
       - MODEL_PATH=/app/models/okla-llama3-8b-chatbot.Q4_K_M.gguf
       - N_CTX=2048
       - N_GPU_LAYERS=0
       - N_THREADS=4
     deploy:
       resources:
         limits:
           memory: 8G
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
       interval: 30s
       timeout: 10s
       start_period: 120s
       retries: 3
     networks:
       - cardealer-network
   ```

2. Verificar que arranca con: `docker compose up chatbotservice llm-server`

---

### PASO 4: Agregar Rutas al Gateway (Ocelot) â±ï¸ ~20 min

**QuÃ© hacer:**

1. Agregar las siguientes rutas a `ocelot.Development.json` y `ocelot.prod.json`:

```json
{
  "Routes": [
    {
      "UpstreamPathTemplate": "/api/chat/sessions",
      "UpstreamHttpMethod": ["POST"],
      "DownstreamPathTemplate": "/api/chat/sessions",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/sessions/{sessionId}/messages",
      "UpstreamHttpMethod": ["POST"],
      "DownstreamPathTemplate": "/api/chat/sessions/{sessionId}/messages",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/sessions/{sessionId}/end",
      "UpstreamHttpMethod": ["POST"],
      "DownstreamPathTemplate": "/api/chat/sessions/{sessionId}/end",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/sessions/{sessionId}/transfer",
      "UpstreamHttpMethod": ["POST"],
      "DownstreamPathTemplate": "/api/chat/sessions/{sessionId}/transfer",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/sessions/{sessionId}",
      "UpstreamHttpMethod": ["GET"],
      "DownstreamPathTemplate": "/api/chat/sessions/{sessionId}",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/sessions/{sessionId}/messages",
      "UpstreamHttpMethod": ["GET"],
      "DownstreamPathTemplate": "/api/chat/sessions/{sessionId}/messages",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }],
      "AuthenticationOptions": { "AuthenticationProviderKey": "Bearer" }
    },
    {
      "UpstreamPathTemplate": "/api/chat/configuration/{configId}",
      "UpstreamHttpMethod": ["GET", "PUT"],
      "DownstreamPathTemplate": "/api/chat/configuration/{configId}",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }]
    },
    {
      "UpstreamPathTemplate": "/api/chat/configuration/dealer/{dealerId}",
      "UpstreamHttpMethod": ["GET"],
      "DownstreamPathTemplate": "/api/chat/configuration/dealer/{dealerId}",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [{ "Host": "chatbotservice", "Port": 8080 }]
    }
  ]
}
```

2. Reiniciar el Gateway: `docker compose restart gateway`
3. Verificar conectividad: `curl http://localhost:18443/api/chat/sessions -H "Authorization: Bearer <token>"`

---

### PASO 5: Implementar Widget de Chat en Frontend (Next.js) â±ï¸ ~4-6 horas

**QuÃ© crear:**

```
frontend/web-next/src/
â”œâ”€â”€ components/
â”‚   â””â”€â”€ chat/
â”‚       â”œâ”€â”€ ChatWidget.tsx           # Widget flotante (bubble + panel)
â”‚       â”œâ”€â”€ ChatBubble.tsx           # BotÃ³n flotante esquina inferior
â”‚       â”œâ”€â”€ ChatPanel.tsx            # Panel de conversaciÃ³n
â”‚       â”œâ”€â”€ ChatMessage.tsx          # Componente de mensaje individual
â”‚       â”œâ”€â”€ ChatInput.tsx            # Input de texto + enviar
â”‚       â”œâ”€â”€ ChatHeader.tsx           # Header con nombre + cerrar
â”‚       â”œâ”€â”€ ChatTypingIndicator.tsx  # "OKLA Bot estÃ¡ escribiendo..."
â”‚       â”œâ”€â”€ ChatVehicleCard.tsx      # Card de vehÃ­culo en chat
â”‚       â”œâ”€â”€ ChatComparisonTable.tsx  # Tabla comparativa
â”‚       â””â”€â”€ index.ts                # Exports
â”œâ”€â”€ services/
â”‚   â””â”€â”€ chatbot.ts                  # API client para ChatbotService
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useChatbot.ts               # Hook principal del chatbot
â””â”€â”€ app/
    â””â”€â”€ (main)/
        â””â”€â”€ layout.tsx              # â† Agregar <ChatWidget /> aquÃ­
```

**CaracterÃ­sticas del widget:**

- ğŸ’¬ BotÃ³n flotante en esquina inferior derecha
- ğŸ“± Panel expandible responsive (mobile: fullscreen, desktop: 400x600px)
- ğŸ”„ Indicador de "escribiendo..."
- ğŸš— Cards de vehÃ­culos inline con imagen, precio, CTA
- ğŸ“Š Tablas comparativas inline
- ğŸ” IntegraciÃ³n con auth (JWT automÃ¡tico)
- ğŸ’¾ Persistencia de sesiÃ³n (localStorage)
- ğŸ”” NotificaciÃ³n de mensajes nuevos
- ğŸ¨ Temas light/dark (sigue el theme de la app)

---

### PASO 6: Implementar Servicio API del Chat (Frontend) â±ï¸ ~2 horas

**API Client (`services/chatbot.ts`):**

```typescript
export const chatbotService = {
  // Sesiones
  startSession: (dealerId: string, channel: string) =>
    api.post("/api/chat/sessions", { dealerId, channel }),

  sendMessage: (sessionId: string, message: string) =>
    api.post(`/api/chat/sessions/${sessionId}/messages`, { content: message }),

  endSession: (sessionId: string) =>
    api.post(`/api/chat/sessions/${sessionId}/end`),

  getSession: (sessionId: string) => api.get(`/api/chat/sessions/${sessionId}`),

  getMessages: (sessionId: string) =>
    api.get(`/api/chat/sessions/${sessionId}/messages`),

  transferToAgent: (sessionId: string, reason: string) =>
    api.post(`/api/chat/sessions/${sessionId}/transfer`, { reason }),
};
```

**Custom Hook (`hooks/useChatbot.ts`):**

- Maneja estado de sesiÃ³n
- Auto-reconnect
- Optimistic UI updates
- Error handling con retry
- Typing indicators
- Sound notifications

---

### PASO 7: Testing Local End-to-End â±ï¸ ~2 horas

**QuÃ© verificar:**

| Test              | Comando                                  | Esperado                |
| ----------------- | ---------------------------------------- | ----------------------- |
| LLM Server health | `curl localhost:8000/health`             | `{"status": "healthy"}` |
| Gateway routing   | `curl localhost:18443/api/chat/sessions` | 401 (sin token)         |
| Start session     | POST con JWT                             | Session ID              |
| Send message      | POST con "Busco una yipeta"              | Respuesta con vehÃ­culos |
| Dominican slang   | "Klk, tiene guagua pela'a?"              | Entiende y responde     |
| PII detection     | Enviar cÃ©dula "001-1234567-8"            | Respuesta sin cÃ©dula    |
| Lead scoring      | Enviar mensajes de compra urgente        | Score > 70              |
| Transfer          | Score alto â†’ transferir                  | Briefing generado       |
| End session       | POST end                                 | Session cerrada         |
| Widget UI         | Abrir chat en browser                    | Chat funcional          |

**Flujo E2E completo:**

```
Browser (localhost:3000)
  â†’ Click chat bubble
  â†’ Widget opens
  â†’ POST /api/chat/sessions (via Next.js rewrite)
  â†’ Gateway (localhost:18443)
  â†’ ChatbotService (localhost:5060)
  â†’ LlmService â†’ llm-server (localhost:8000)
  â†’ Llama 3 genera respuesta
  â†’ Response back to widget
```

---

### PASO 8: Limpiar Artefactos de Dialogflow â±ï¸ ~30 min

**Archivos a eliminar:**

- `DialogflowService.cs.ELIMINATED`

**CÃ³digo a renombrar (opcional, backward compatible):**

- `DialogflowIntentName` â†’ mantener por compatibilidad DB
- Actualizar comentarios y Swagger descriptions

---

### PASO 9: Deploy a ProducciÃ³n (DOKS) â±ï¸ ~2 horas

**Pasos:**

1. **Build y push imÃ¡genes Docker:**

   ```bash
   # ChatbotService
   docker build -t ghcr.io/gregorymorenoiem/okla-chatbotservice:latest ./backend/ChatbotService
   docker push ghcr.io/gregorymorenoiem/okla-chatbotservice:latest

   # LLM Server
   docker build -t ghcr.io/gregorymorenoiem/okla-llm-server:latest ./backend/ChatbotService/LlmServer
   docker push ghcr.io/gregorymorenoiem/okla-llm-server:latest
   ```

2. **Subir modelo a HuggingFace Hub:**

   ```bash
   huggingface-cli upload gregorymorenoiem/okla-llama3-8b-chatbot \
     okla-llama3-8b-chatbot.Q4_K_M.gguf
   ```

3. **Deploy K8s:**

   ```bash
   # LLM Server
   kubectl apply -f backend/ChatbotService/k8s/llm-server.yaml

   # Esperar que el Job descargue el modelo (~5-10 min)
   kubectl wait --for=condition=complete job/download-llm-model -n okla --timeout=600s

   # ChatbotService (en deployments.yaml existente)
   kubectl apply -f k8s/deployments.yaml

   # Actualizar Gateway ConfigMap
   kubectl delete configmap gateway-config -n okla
   kubectl create configmap gateway-config \
     --from-file=ocelot.json=backend/Gateway/Gateway.Api/ocelot.prod.json -n okla
   kubectl rollout restart deployment/gateway -n okla
   ```

4. **Verificar en producciÃ³n:**
   ```bash
   kubectl get pods -n okla | grep -E "chatbot|llm"
   kubectl logs -f deployment/chatbotservice -n okla
   kubectl logs -f deployment/llm-server -n okla
   ```

---

### PASO 10: Monitoreo Post-Deploy (FASE 5) â±ï¸ Continuo

**Setup inicial:**

1. Verificar mÃ©tricas Prometheus: `kubectl port-forward svc/llm-server 8000:8000 -n okla` â†’ `/metrics`
2. Importar dashboard Grafana desde `FASE_5_MEJORA_CONTINUA/monitoring/grafana_dashboard.json`
3. Configurar alertas:
   - Latencia > 10s â†’ Warning
   - Error rate > 5% â†’ Critical
   - Model not loaded â†’ Critical

**Cadencia operativa:**
| Frecuencia | AcciÃ³n |
|---|---|
| **Diaria** | Revisar logs y mÃ©tricas bÃ¡sicas |
| **Semanal** | Evaluar calidad con `evaluation/evaluate_model.py` |
| **Semanal** | Analizar feedback con `feedback/feedback_collector.py` |
| **Mensual** | Detectar drift con `monitoring/drift_detector.py` |
| **Trimestral** | Re-entrenar modelo si mÃ©tricas bajan |

---

## ğŸ“ Arquitectura Final

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USUARIO (Browser)                        â”‚
â”‚                     https://okla.com.do                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Next.js 14)                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ChatWidget   â”‚  â”‚  useChatbot  â”‚  â”‚  chatbotService.ts   â”‚  â”‚
â”‚  â”‚  (flotante)   â”‚â†’â”‚  (hook)      â”‚â†’â”‚  (API client)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                  â”‚              â”‚
â”‚                    Next.js Rewrite: /api/* â†’ gateway:8080       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GATEWAY (Ocelot)                             â”‚
â”‚                    /api/chat/* â†’ chatbotservice:8080            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CHATBOT SERVICE (.NET 8)                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PII     â”‚   â”‚ Quick      â”‚   â”‚    LlmService            â”‚  â”‚
â”‚  â”‚ Detectionâ”‚â†’ â”‚ Response   â”‚â†’ â”‚  (HTTP â†’ llm-server)     â”‚  â”‚
â”‚  â”‚ (regex)  â”‚   â”‚ Check      â”‚   â”‚  + Legal Audit           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  + Lead Scoring          â”‚  â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”‚ PostgreSQL (sessions, messages, leads, config)               â”‚
â”‚  â”‚ Redis (cache, rate limiting)                                 â”‚
â”‚  â”‚ RabbitMQ (events: lead.created, session.ended)               â”‚
â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM SERVER (FastAPI)                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  llama-cpp-python                                        â”‚   â”‚
â”‚  â”‚  Model: okla-llama3-8b-chatbot.Q4_K_M.gguf (~4.7 GB)   â”‚   â”‚
â”‚  â”‚  Context: 2048 tokens                                    â”‚   â”‚
â”‚  â”‚  Quantization: Q4_K_M (4-bit)                           â”‚   â”‚
â”‚  â”‚  API: OpenAI-compatible /v1/chat/completions             â”‚   â”‚
â”‚  â”‚  Metrics: Prometheus /metrics                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ Timeline Estimado

| Paso | Tarea                    |  Tiempo  | Dependencia |
| :--: | ------------------------ | :------: | :---------: |
|  1   | Entrenar modelo en Colab |   ~3h    | Dataset âœ…  |
|  2   | Probar LLM Server local  |  ~30min  |   Paso 1    |
|  3   | Docker Compose principal |  ~15min  |      â€”      |
|  4   | Gateway routes (Ocelot)  |  ~20min  |      â€”      |
|  5   | Widget chat frontend     |  ~4-6h   |      â€”      |
|  6   | Servicio API frontend    |   ~2h    |      â€”      |
|  7   | Testing E2E local        |   ~2h    |  Pasos 1-6  |
|  8   | Limpiar Dialogflow       |  ~30min  |      â€”      |
|  9   | Deploy a DOKS            |   ~2h    |  Pasos 1-8  |
|  10  | Monitoreo post-deploy    | Continuo |   Paso 9    |

**Total estimado: ~15-17 horas** (2-3 dÃ­as de trabajo)

---

## ğŸ’° Costos Estimados

| Recurso                                   | Costo                     |
| ----------------------------------------- | ------------------------- |
| **Colab Pro** (para entrenar)             | $10/mes (o gratis con T4) |
| **Droplet 8GB** (para LLM Server en DOKS) | ~$48/mes (s-4vcpu-8gb)    |
| **Block Storage 10Gi** (para modelo GGUF) | ~$1/mes                   |
| **Total mensual adicional**               | ~$49/mes                  |

---

## ğŸ”§ Requisitos Previos

- [ ] Cuenta Google Colab (con GPU T4 o superior)
- [ ] HuggingFace account + token (para descargar Llama 3)
- [ ] Meta Llama 3 access granted (solicitar en huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- [ ] Docker Desktop con al menos 10GB RAM disponible
- [ ] kubectl configurado para okla-cluster
- [ ] ghcr.io access configurado

---

## â“ Decisiones Pendientes

| DecisiÃ³n                     | Opciones                                                | RecomendaciÃ³n                                     |
| ---------------------------- | ------------------------------------------------------- | ------------------------------------------------- |
| **Â¿CuÃ¡ndo mostrar el chat?** | Siempre / Solo en pÃ¡ginas de vehÃ­culos / Solo logged in | Solo en pÃ¡ginas de vehÃ­culos + dealer             |
| **Â¿Chat anÃ³nimo?**           | SÃ­ (con lÃ­mites) / Solo autenticados                    | AnÃ³nimo con lÃ­mite de 5 mensajes â†’ pedir registro |
| **Â¿Sonido de notificaciÃ³n?** | SÃ­ / No                                                 | SÃ­, configurable                                  |
| **Â¿WhatsApp integration?**   | SÃ­ (Fase futura) / No                                   | Fase futura via Twilio                            |
| **Â¿Multi-idioma?**           | Solo espaÃ±ol / EspaÃ±ol + inglÃ©s                         | Solo espaÃ±ol (RD market)                          |

---

_Documento generado para el equipo de desarrollo OKLA â€” Febrero 2026_
