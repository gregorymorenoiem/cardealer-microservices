# üß† FASE 3 ‚Äî Fine-Tuning Llama 3 8B con QLoRA

> **Proyecto:** OKLA ‚Äî Marketplace de Veh√≠culos (Rep√∫blica Dominicana)  
> **√öltima actualizaci√≥n:** Febrero 15, 2026

---

## ‚ö†Ô∏è DIRECTIVA CR√çTICA ‚Äî REEMPLAZO DE DIALOGFLOW

> El ChatbotService actual basado en Google Dialogflow ES **DEBE SER COMPLETAMENTE ELIMINADO**  
> y reemplazado por el modelo LLM fine-tuned producido en esta fase.

---

## üìã Contenido

| Archivo                      | Descripci√≥n                                        |
| ---------------------------- | -------------------------------------------------- |
| `okla_finetune_llama3.ipynb` | Notebook principal ‚Äî ejecutar en Colab via VS Code |

## üîß Pre-requisitos

### 1. Extensi√≥n Colab en VS Code

La extensi√≥n **Google Colab** (`Google.colab`) debe estar instalada en VS Code.

### 2. Cuenta de HuggingFace

- Crear cuenta en [huggingface.co](https://huggingface.co)
- Solicitar acceso a [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- Generar token en [Settings ‚Üí Tokens](https://huggingface.co/settings/tokens)

### 3. Dataset de FASE 2

Los archivos JSONL generados en FASE 2:

- `okla_train.jsonl` (80% ‚Äî ~2,400 conversaciones)
- `okla_eval.jsonl` (10% ‚Äî ~300 conversaciones)
- `okla_test.jsonl` (10% ‚Äî ~300 conversaciones)

---

## üöÄ C√≥mo Ejecutar

### Paso 1: Abrir notebook

Abre `okla_finetune_llama3.ipynb` en VS Code.

### Paso 2: Conectar a Colab

1. Click **"Select Kernel"** (esquina superior derecha)
2. Selecciona **"Colab"** ‚Üí **"New Colab Server"**
3. Inicia sesi√≥n con tu cuenta de Google
4. El runtime se conecta autom√°ticamente con **GPU T4 (16GB)**

### Paso 3: Ejecutar celdas

Ejecuta celda por celda en orden:

| #   | Celda                 | Tiempo estimado |
| --- | --------------------- | --------------- |
| 1   | Verificar GPU         | ~5 seg          |
| 2   | Instalar dependencias | ~3-5 min        |
| 3   | Subir dataset         | ~1 min (upload) |
| 4   | Cargar y formatear    | ~2 min          |
| 5   | Cargar modelo 4-bit   | ~5-8 min        |
| 6   | Configurar QLoRA      | ~10 seg         |
| 7   | Entrenar              | **45-120 min**  |
| 8   | Evaluar               | ~5-10 min       |
| 9   | Guardar modelo        | ~5-10 min       |
| 10  | Exportar GGUF         | ~10-15 min      |
| 11  | Subir a Drive         | ~5 min          |

**Tiempo total estimado: ~2-3 horas**

---

## üìä Especificaciones T√©cnicas

| Componente                 | Configuraci√≥n                                                 |
| -------------------------- | ------------------------------------------------------------- |
| **Modelo base**            | Meta-Llama-3-8B-Instruct                                      |
| **Cuantizaci√≥n**           | NF4 (4-bit) con doble cuantizaci√≥n                            |
| **M√©todo**                 | QLoRA (PEFT)                                                  |
| **LoRA rank**              | 16                                                            |
| **LoRA alpha**             | 32                                                            |
| **Target modules**         | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj |
| **Par√°metros entrenables** | ~1.5-2% del total                                             |
| **Optimizer**              | Paged AdamW 8-bit                                             |
| **Learning rate**          | 2e-4 con cosine annealing                                     |
| **Batch size**             | 2 √ó 8 gradient accumulation = 16 efectivo                     |
| **Epochs**                 | 3                                                             |
| **Precision**              | FP16 (T4)                                                     |
| **Max seq length**         | Din√°mico (P95 del dataset)                                    |
| **VRAM requerida**         | ~12-14 GB (cabe en T4 16GB)                                   |

---

## üì¶ Artefactos de Salida

Despu√©s del entrenamiento, tendr√°s:

| Artefacto            | Tama√±o     | Uso                                |
| -------------------- | ---------- | ---------------------------------- |
| **LoRA Adapters**    | ~50-100 MB | Para cargar sobre modelo base      |
| **Merged Model**     | ~15 GB     | Modelo completo en FP16            |
| **GGUF Q4_K_M**      | ~4.7 GB    | **Producci√≥n** ‚Äî CPU con 6-8GB RAM |
| **Training Metrics** | <1 MB      | Loss, perplexity, JSON accuracy    |

### Ubicaci√≥n en Google Drive

```
MyDrive/OKLA/models/
‚îú‚îÄ‚îÄ okla-llama3-adapter/        # LoRA adapters
‚îú‚îÄ‚îÄ okla-llama3-8b-q4_k_m.gguf # Modelo para producci√≥n
‚îî‚îÄ‚îÄ training_metrics/           # M√©tricas
```

---

## üéØ M√©tricas de Calidad Esperadas

| M√©trica              | Umbral aceptable | Ideal |
| -------------------- | ---------------- | ----- |
| **Eval loss**        | < 1.5            | < 0.8 |
| **Perplexity**       | < 5.0            | < 2.5 |
| **JSON v√°lido**      | > 85%            | > 95% |
| **Campos completos** | > 80%            | > 90% |
| **Intent correcto**  | > 70%            | > 85% |

---

## ‚ö†Ô∏è Colab Free ‚Äî Limitaciones

| Limitaci√≥n       | Impacto                  | Mitigaci√≥n                     |
| ---------------- | ------------------------ | ------------------------------ |
| Sesi√≥n max 12h   | Puede desconectarse      | Checkpoints cada 100 steps     |
| GPU intermitente | Puede no haber GPU       | Reintentar m√°s tarde           |
| RAM ~12.7GB      | Merge puede fallar       | Skip merge, usar solo adapters |
| Disco ~78GB      | Suficiente para Llama 8B | Limpiar antes de GGUF export   |

---

## üîú Siguiente: FASE 4 ‚Äî Deployment

El GGUF generado aqu√≠ se usar√° en FASE 4 para:

1. Servir con `llama.cpp` server en Docker/K8s
2. Reemplazar `DialogflowService.cs` por `LlmService.cs`
3. Nuevo endpoint `/api/chatbot/llm/completions`
