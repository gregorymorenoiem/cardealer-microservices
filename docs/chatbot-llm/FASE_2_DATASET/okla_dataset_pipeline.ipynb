{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987389df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'julia 1.11'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:6838b6c3-3535-4156-aff9-dc24cb569ea2"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. SETUP Y CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Detectar entorno\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"üñ•Ô∏è  Entorno: {'Google Colab' if IN_COLAB else 'Local (VS Code)'}\")\n",
    "\n",
    "# Resolver paths seg√∫n entorno\n",
    "if IN_COLAB:\n",
    "    # ‚îÄ‚îÄ En Colab: los archivos locales NO existen ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Montar Google Drive\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "    # Intentar usar archivos desde Drive primero\n",
    "    DRIVE_FASE2 = Path('/content/drive/MyDrive/OKLA/chatbot-llm/FASE_2_DATASET')\n",
    "    BASE_DIR = Path('/content/FASE_2_DATASET')\n",
    "    BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # Archivos necesarios para ejecutar el pipeline\n",
    "    REQUIRED_FILES = [\n",
    "        'seed_vehicles.json',\n",
    "        'seed_dealers.json',\n",
    "        'conversation_templates.py',\n",
    "        'generate_dataset.py',\n",
    "        'validate_dataset.py',\n",
    "    ]\n",
    "    REQUIRED_DIRS = {\n",
    "        'augmentation': ['paraphrase_variants.py'],\n",
    "    }\n",
    "\n",
    "    # Verificar si los archivos est√°n en Drive\n",
    "    if DRIVE_FASE2.exists() and (DRIVE_FASE2 / 'seed_vehicles.json').exists():\n",
    "        print(f\"‚úÖ Archivos encontrados en Drive: {DRIVE_FASE2}\")\n",
    "        import shutil\n",
    "        # Copiar todo al runtime\n",
    "        for f in REQUIRED_FILES:\n",
    "            src = DRIVE_FASE2 / f\n",
    "            if src.exists():\n",
    "                shutil.copy2(src, BASE_DIR / f)\n",
    "                print(f\"   ‚úÖ {f}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  {f} no encontrado en Drive\")\n",
    "        # Copiar directorios\n",
    "        for dirname, dir_files in REQUIRED_DIRS.items():\n",
    "            src_dir = DRIVE_FASE2 / dirname\n",
    "            dest_dir = BASE_DIR / dirname\n",
    "            dest_dir.mkdir(exist_ok=True)\n",
    "            for df in dir_files:\n",
    "                src = src_dir / df\n",
    "                if src.exists():\n",
    "                    shutil.copy2(src, dest_dir / df)\n",
    "                    print(f\"   ‚úÖ {dirname}/{df}\")\n",
    "    else:\n",
    "        # Archivos no est√°n en Drive ‚Äî pedir upload manual\n",
    "        print(f\"‚ö†Ô∏è  Archivos de FASE 2 no encontrados en Drive.\")\n",
    "        print(f\"   Esperado en: {DRIVE_FASE2}\")\n",
    "        print()\n",
    "        print(\"   üì§ Subiendo archivos desde tu m√°quina...\")\n",
    "        print(\"   Selecciona TODOS estos archivos cuando se abra el di√°logo:\")\n",
    "        print(\"   (est√°n en docs/chatbot-llm/FASE_2_DATASET/)\")\n",
    "        print()\n",
    "        for f in REQUIRED_FILES:\n",
    "            print(f\"   üìÑ {f}\")\n",
    "        for dirname, dir_files in REQUIRED_DIRS.items():\n",
    "            for df in dir_files:\n",
    "                print(f\"   üìÑ {dirname}/{df}\")\n",
    "        print()\n",
    "\n",
    "        from google.colab import files  # type: ignore\n",
    "        print(\"üîº Selecciona los archivos (seed_vehicles.json, seed_dealers.json,\")\n",
    "        print(\"   conversation_templates.py, generate_dataset.py, validate_dataset.py):\")\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        for fname, content in uploaded.items():\n",
    "            dest = BASE_DIR / fname\n",
    "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(dest, 'wb') as f:\n",
    "                f.write(content)\n",
    "            print(f\"   ‚úÖ {fname} ‚Üí {dest}\")\n",
    "\n",
    "        # Verificar si falta augmentation\n",
    "        aug_dir = BASE_DIR / 'augmentation'\n",
    "        if not (aug_dir / 'paraphrase_variants.py').exists():\n",
    "            aug_dir.mkdir(exist_ok=True)\n",
    "            print()\n",
    "            print(\"üîº Ahora selecciona: augmentation/paraphrase_variants.py\")\n",
    "            uploaded2 = files.upload()\n",
    "            for fname, content in uploaded2.items():\n",
    "                with open(aug_dir / fname, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"   ‚úÖ augmentation/{fname}\")\n",
    "\n",
    "    # Verificar que todo est√° listo\n",
    "    missing = [f for f in REQUIRED_FILES if not (BASE_DIR / f).exists()]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ùå Archivos faltantes: {missing}\")\n",
    "        raise FileNotFoundError(f\"Faltan archivos: {missing}\")\n",
    "\n",
    "else:\n",
    "    # ‚îÄ‚îÄ Local: usar ruta relativa al notebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    BASE_DIR = Path('.').resolve()\n",
    "    if not (BASE_DIR / 'seed_vehicles.json').exists():\n",
    "        for candidate in [BASE_DIR, BASE_DIR.parent / 'FASE_2_DATASET']:\n",
    "            if (candidate / 'seed_vehicles.json').exists():\n",
    "                BASE_DIR = candidate\n",
    "                break\n",
    "\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Agregar BASE_DIR al path para imports\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "print(f\"\\nüìÇ Base:   {BASE_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "\n",
    "# Verificar archivos clave\n",
    "for f in ['seed_vehicles.json', 'seed_dealers.json', 'conversation_templates.py', 'generate_dataset.py']:\n",
    "    status = '‚úÖ' if (BASE_DIR / f).exists() else '‚ùå'\n",
    "    print(f\"   {status} {f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd26b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este c√≥digo ya est√° manejado en CELL INDEX 0\n",
    "# No necesitas remontarlo aqu√≠\n",
    "\n",
    "# Si necesitas acceder a Drive en esta celda:\n",
    "if IN_COLAB:\n",
    "\t# Ya est√° montado en CELL INDEX 0\n",
    "\tdrive_path = Path('/content/drive/MyDrive')\n",
    "\tprint(f\"‚úÖ Drive disponible en: {drive_path}\")\n",
    "else:\n",
    "\tprint(\"‚ö†Ô∏è No est√°s en Colab ‚Äî usando archivos locales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1b. SUBIR DATASET A GOOGLE DRIVE (desde Colab)\n",
    "# ============================================================\n",
    "# Crea la estructura OKLA/dataset/ en Drive y sube los JSONL\n",
    "# que generaste localmente en FASE 2.\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "drive_base = Path('/content/drive/MyDrive')\n",
    "drive_dataset = drive_base / 'OKLA' / 'dataset'\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "drive_dataset.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÇ Carpeta creada/verificada: {drive_dataset}\")\n",
    "\n",
    "# Verificar si ya hay archivos\n",
    "JSONL_FILES = ['okla_train.jsonl', 'okla_eval.jsonl', 'okla_test.jsonl']\n",
    "existing = [f for f in JSONL_FILES if (drive_dataset / f).exists()]\n",
    "\n",
    "if len(existing) == len(JSONL_FILES):\n",
    "    print(f\"\\n‚úÖ Dataset ya est√° en Drive:\")\n",
    "    for f in JSONL_FILES:\n",
    "        fp = drive_dataset / f\n",
    "        lines = sum(1 for _ in open(fp))\n",
    "        size_mb = fp.stat().st_size / 1024 / 1024\n",
    "        print(f\"   ‚úÖ {f}: {lines} conv. ({size_mb:.1f} MB)\")\n",
    "    print(f\"\\nüöÄ Listo ‚Äî puedes ir a FASE_3 directamente.\")\n",
    "else:\n",
    "    if existing:\n",
    "        print(f\"\\n‚ö†Ô∏è Solo {len(existing)}/{len(JSONL_FILES)} archivos encontrados:\")\n",
    "        for f in existing:\n",
    "            print(f\"   ‚úÖ {f}\")\n",
    "        missing = [f for f in JSONL_FILES if f not in existing]\n",
    "        for f in missing:\n",
    "            print(f\"   ‚ùå {f}\")\n",
    "    \n",
    "    # Subir archivos via upload dialog\n",
    "    print(f\"\\nüì§ Selecciona los 3 archivos JSONL de tu m√°quina:\")\n",
    "    print(f\"   (est√°n en docs/chatbot-llm/FASE_2_DATASET/output/)\")\n",
    "    print()\n",
    "    \n",
    "    from google.colab import files  # type: ignore\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for fname, content in uploaded.items():\n",
    "        dest = drive_dataset / fname\n",
    "        with open(dest, 'wb') as f:\n",
    "            f.write(content)\n",
    "        size_mb = len(content) / 1024 / 1024\n",
    "        print(f\"   ‚úÖ {fname} ‚Üí {dest} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Verificar resultado final\n",
    "    print(f\"\\nüìä Verificaci√≥n final:\")\n",
    "    all_ok = True\n",
    "    for f in JSONL_FILES:\n",
    "        fp = drive_dataset / f\n",
    "        if fp.exists():\n",
    "            lines = sum(1 for _ in open(fp))\n",
    "            print(f\"   ‚úÖ {f}: {lines} conversaciones\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {f}: FALTA ‚Äî vuelve a ejecutar esta celda\")\n",
    "            all_ok = False\n",
    "    \n",
    "    if all_ok:\n",
    "        print(f\"\\n‚úÖ Dataset completo en Drive: {drive_dataset}\")\n",
    "        print(f\"üöÄ Ahora abre FASE_3_TRAINING/okla_finetune_llama3.ipynb\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Faltan archivos. Ejecuta esta celda otra vez.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae26db",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Cargar Datos Semilla\n",
    "\n",
    "Carga los 50 veh√≠culos y 5 dealers del mercado dominicano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. CARGAR SEED DATA\n",
    "# ============================================================\n",
    "\n",
    "# Cargar veh√≠culos\n",
    "vehicles_path = BASE_DIR / 'seed_vehicles.json'\n",
    "with open(vehicles_path) as f:\n",
    "    vehicles = json.load(f)\n",
    "\n",
    "print(f\"üöó {len(vehicles)} veh√≠culos cargados\")\n",
    "print(f\"\\nüìä Distribuci√≥n por marca:\")\n",
    "makes = Counter(v['make'] for v in vehicles)\n",
    "for make, count in makes.most_common(10):\n",
    "    bar = '‚ñà' * count\n",
    "    print(f\"   {make:15s} {bar} ({count})\")\n",
    "\n",
    "print(f\"\\nüí∞ Rango de precios:\")\n",
    "prices = [v['price'] for v in vehicles]\n",
    "print(f\"   Min: RD${min(prices):,.0f}\")\n",
    "print(f\"   Max: RD${max(prices):,.0f}\")\n",
    "print(f\"   Avg: RD${sum(prices)/len(prices):,.0f}\")\n",
    "\n",
    "# Cargar dealers\n",
    "dealers_path = BASE_DIR / 'seed_dealers.json'\n",
    "with open(dealers_path) as f:\n",
    "    dealers = json.load(f)\n",
    "\n",
    "print(f\"\\nüè™ {len(dealers)} dealers cargados:\")\n",
    "for d in dealers:\n",
    "    print(f\"   ‚Ä¢ {d['name']} (Bot: {d['botName']}) ‚Äî {d['location']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Seed data lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5bf90",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Configuraci√≥n de Generaci√≥n\n",
    "\n",
    "Ajusta los par√°metros para la generaci√≥n del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "\n",
    "# ‚îÄ‚îÄ Par√°metros ajustables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "NUM_CONVERSATIONS = 3000      # Total de conversaciones a generar\n",
    "TRAIN_RATIO = 0.80            # 80% train\n",
    "EVAL_RATIO = 0.10             # 10% eval\n",
    "TEST_RATIO = 0.10             # 10% test\n",
    "SEED = 42                     # Semilla para reproducibilidad\n",
    "\n",
    "# Distribuci√≥n de tipos de conversaci√≥n\n",
    "SINGLE_TURN_PCT = 0.15        # 15% conversaciones de 1 turno\n",
    "SHORT_MULTI_PCT = 0.60        # 60% multi-turno corto (2-4 turnos)\n",
    "LONG_MULTI_PCT = 0.25         # 25% multi-turno largo (5-8 turnos)\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# Calcular splits\n",
    "n_train = int(NUM_CONVERSATIONS * TRAIN_RATIO)\n",
    "n_eval = int(NUM_CONVERSATIONS * EVAL_RATIO)\n",
    "n_test = NUM_CONVERSATIONS - n_train - n_eval\n",
    "\n",
    "print(f\"‚öôÔ∏è Configuraci√≥n:\")\n",
    "print(f\"   Conversaciones: {NUM_CONVERSATIONS:,}\")\n",
    "print(f\"   Train: {n_train:,} ({TRAIN_RATIO*100:.0f}%)\")\n",
    "print(f\"   Eval:  {n_eval:,} ({EVAL_RATIO*100:.0f}%)\")\n",
    "print(f\"   Test:  {n_test:,} ({TEST_RATIO*100:.0f}%)\")\n",
    "print(f\"   Seed:  {SEED}\")\n",
    "print(f\"\\n   Single-turn: {SINGLE_TURN_PCT*100:.0f}%\")\n",
    "print(f\"   Multi-turn short (2-4): {SHORT_MULTI_PCT*100:.0f}%\")\n",
    "print(f\"   Multi-turn long (5-8): {LONG_MULTI_PCT*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50d0a1",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Importar Templates y Generar\n",
    "\n",
    "Importa `conversation_templates.py` y ejecuta la generaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41347e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. IMPORTAR TEMPLATES Y GENERAR DATASET\n",
    "# ============================================================\n",
    "from conversation_templates import (\n",
    "    INTENT_REGISTRY,\n",
    "    MULTI_TURN_CHAINS,\n",
    "    BODY_TYPE_SLANG,\n",
    "    PRICE_EXPRESSIONS,\n",
    "    AFFIRMATIVES,\n",
    ")\n",
    "\n",
    "print(f\"üìã Templates cargados:\")\n",
    "print(f\"   Intents: {len(INTENT_REGISTRY)}\")\n",
    "for intent_name, intent_data in INTENT_REGISTRY.items():\n",
    "    n_templates = len(intent_data.get('user_templates', []))\n",
    "    print(f\"   ‚Ä¢ {intent_name}: {n_templates} templates\")\n",
    "print(f\"   Multi-turn chains: {len(MULTI_TURN_CHAINS)}\")\n",
    "print(f\"   Body type slang: {len(BODY_TYPE_SLANG)} entries\")\n",
    "print(f\"   Price expressions: {len(PRICE_EXPRESSIONS)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4b. EJECUTAR GENERACI√ìN\n",
    "# ============================================================\n",
    "# Importar el generador\n",
    "from generate_dataset import (\n",
    "    generate_single_turn,\n",
    "    generate_multi_turn,\n",
    "    build_system_prompt,\n",
    ")\n",
    "\n",
    "conversations = []\n",
    "intent_counts = Counter()\n",
    "turn_counts = []\n",
    "\n",
    "print(f\"üîÑ Generando {NUM_CONVERSATIONS:,} conversaciones...\")\n",
    "print()\n",
    "\n",
    "for i in range(NUM_CONVERSATIONS):\n",
    "    # Seleccionar dealer y veh√≠culos aleatorios\n",
    "    dealer = random.choice(dealers)\n",
    "    dealer_vehicles = random.sample(vehicles, min(random.randint(5, 15), len(vehicles)))\n",
    "    system_prompt = build_system_prompt(dealer, dealer_vehicles)\n",
    "    \n",
    "    # Decidir tipo de conversaci√≥n\n",
    "    r = random.random()\n",
    "    if r < SINGLE_TURN_PCT:\n",
    "        conv = generate_single_turn(system_prompt, dealer, dealer_vehicles)\n",
    "        conv_type = 'single'\n",
    "    elif r < SINGLE_TURN_PCT + SHORT_MULTI_PCT:\n",
    "        conv = generate_multi_turn(system_prompt, dealer, dealer_vehicles, \n",
    "                                   min_turns=2, max_turns=4)\n",
    "        conv_type = 'short_multi'\n",
    "    else:\n",
    "        conv = generate_multi_turn(system_prompt, dealer, dealer_vehicles,\n",
    "                                   min_turns=5, max_turns=8)\n",
    "        conv_type = 'long_multi'\n",
    "    \n",
    "    conversations.append(conv)\n",
    "    \n",
    "    # Track stats\n",
    "    n_turns = len([m for m in conv['messages'] if m['role'] == 'user'])\n",
    "    turn_counts.append(n_turns)\n",
    "    for m in conv['messages']:\n",
    "        if m['role'] == 'assistant':\n",
    "            try:\n",
    "                parsed = json.loads(m['content'])\n",
    "                intent_counts[parsed.get('intent', 'unknown')] += 1\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                pass\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"   [{i+1:,}/{NUM_CONVERSATIONS:,}] generadas...\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(conversations):,} conversaciones generadas\")\n",
    "print(f\"   Turnos promedio: {sum(turn_counts)/len(turn_counts):.1f}\")\n",
    "print(f\"   Intents √∫nicos: {len(intent_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48afab4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Visualizar Estad√≠sticas\n",
    "\n",
    "Distribuci√≥n de intents, turnos y longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. ESTAD√çSTICAS Y VISUALIZACI√ìN\n",
    "# ============================================================\n",
    "import statistics\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä ESTAD√çSTICAS DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‚îÄ‚îÄ Distribuci√≥n de intents ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(f\"\\nüéØ Distribuci√≥n de Intents ({len(intent_counts)} √∫nicos):\")\n",
    "max_count = max(intent_counts.values()) if intent_counts else 1\n",
    "for intent, count in intent_counts.most_common():\n",
    "    bar_len = int(30 * count / max_count)\n",
    "    bar = '‚ñà' * bar_len\n",
    "    pct = 100 * count / sum(intent_counts.values())\n",
    "    print(f\"   {intent:25s} {bar:30s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# ‚îÄ‚îÄ Distribuci√≥n de turnos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(f\"\\nüí¨ Distribuci√≥n de Turnos por Conversaci√≥n:\")\n",
    "turn_dist = Counter(turn_counts)\n",
    "for turns in sorted(turn_dist.keys()):\n",
    "    count = turn_dist[turns]\n",
    "    bar = '‚ñà' * int(30 * count / max(turn_dist.values()))\n",
    "    print(f\"   {turns} turnos: {bar:30s} {count:4d}\")\n",
    "\n",
    "print(f\"\\n   Media:   {statistics.mean(turn_counts):.1f} turnos\")\n",
    "print(f\"   Mediana: {statistics.median(turn_counts):.0f} turnos\")\n",
    "print(f\"   Std:     {statistics.stdev(turn_counts):.1f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Longitudes de mensajes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "user_lengths = []\n",
    "bot_lengths = []\n",
    "for conv in conversations:\n",
    "    for m in conv['messages']:\n",
    "        if m['role'] == 'user':\n",
    "            user_lengths.append(len(m['content']))\n",
    "        elif m['role'] == 'assistant':\n",
    "            bot_lengths.append(len(m['content']))\n",
    "\n",
    "print(f\"\\nüìè Longitudes de Mensajes (caracteres):\")\n",
    "print(f\"   User    ‚Äî Media: {statistics.mean(user_lengths):.0f}, Min: {min(user_lengths)}, Max: {max(user_lengths)}\")\n",
    "print(f\"   Bot     ‚Äî Media: {statistics.mean(bot_lengths):.0f}, Min: {min(bot_lengths)}, Max: {max(bot_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e3e44",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Split y Guardar Dataset\n",
    "\n",
    "Divide en train/eval/test y guarda como JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. SPLIT Y GUARDAR\n",
    "# ============================================================\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(conversations)\n",
    "\n",
    "# Split\n",
    "train_data = conversations[:n_train]\n",
    "eval_data = conversations[n_train:n_train + n_eval]\n",
    "test_data = conversations[n_train + n_eval:]\n",
    "\n",
    "# Guardar JSONL\n",
    "def save_jsonl(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    return len(data)\n",
    "\n",
    "splits = {\n",
    "    'okla_train.jsonl': train_data,\n",
    "    'okla_eval.jsonl': eval_data,\n",
    "    'okla_test.jsonl': test_data,\n",
    "}\n",
    "\n",
    "print(\"üíæ Guardando dataset...\")\n",
    "for fname, data in splits.items():\n",
    "    path = OUTPUT_DIR / fname\n",
    "    n = save_jsonl(data, path)\n",
    "    size_kb = path.stat().st_size / 1024\n",
    "    print(f\"   ‚úÖ {fname}: {n:,} conversaciones ({size_kb:.0f} KB)\")\n",
    "\n",
    "# Guardar stats\n",
    "stats = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'total': len(conversations),\n",
    "    'train': len(train_data),\n",
    "    'eval': len(eval_data),\n",
    "    'test': len(test_data),\n",
    "    'intents': dict(intent_counts.most_common()),\n",
    "    'avg_turns': round(statistics.mean(turn_counts), 1),\n",
    "    'seed': SEED,\n",
    "}\n",
    "with open(OUTPUT_DIR / 'stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "print(f\"   ‚úÖ stats.json\")\n",
    "\n",
    "print(f\"\\nüìÇ Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b380b64",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Validaci√≥n\n",
    "\n",
    "Valida la estructura y calidad del dataset generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828612b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. VALIDACI√ìN\n",
    "# ============================================================\n",
    "from validate_dataset import validate_conversation, check_quality\n",
    "\n",
    "print(\"üîç Validando dataset...\")\n",
    "print()\n",
    "\n",
    "total_valid = 0\n",
    "total_errors = 0\n",
    "error_types = Counter()\n",
    "\n",
    "for split_name, split_file in [('train', 'okla_train.jsonl'), ('eval', 'okla_eval.jsonl'), ('test', 'okla_test.jsonl')]:\n",
    "    path = OUTPUT_DIR / split_file\n",
    "    valid = 0\n",
    "    errors = 0\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                conv = json.loads(line)\n",
    "                result = validate_conversation(conv, line_num)\n",
    "                if result['valid']:\n",
    "                    valid += 1\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    for err in result.get('errors', []):\n",
    "                        error_types[err] += 1\n",
    "            except json.JSONDecodeError:\n",
    "                errors += 1\n",
    "                error_types['invalid_json'] += 1\n",
    "    \n",
    "    total_valid += valid\n",
    "    total_errors += errors\n",
    "    status = '‚úÖ' if errors == 0 else '‚ö†Ô∏è'\n",
    "    print(f\"   {status} {split_name}: {valid}/{valid+errors} v√°lidas\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "total = total_valid + total_errors\n",
    "pct = 100 * total_valid / total if total > 0 else 0\n",
    "print(f\"üìä Total: {total_valid:,}/{total:,} v√°lidas ({pct:.1f}%)\")\n",
    "\n",
    "if total_errors > 0:\n",
    "    print(f\"\\n‚ùå Errores encontrados:\")\n",
    "    for err, count in error_types.most_common():\n",
    "        print(f\"   ‚Ä¢ {err}: {count}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ DATASET 100% V√ÅLIDO ‚Äî Listo para FASE 3 (Fine-tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0fca6",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Subir a Google Drive (para FASE 3)\n",
    "\n",
    "Si ejecutaste localmente, sube los JSONL a Drive para usarlos en el notebook de fine-tuning.\n",
    "\n",
    "> üí° Si ejecutaste en Colab con Drive montado, los archivos ya est√°n accesibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25081ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. SUBIR A GOOGLE DRIVE (si ejecutaste localmente)\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Ya en Colab con Drive montado\n",
    "    DRIVE_DEST = Path('/content/drive/MyDrive/OKLA/dataset')\n",
    "    DRIVE_DEST.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for f in OUTPUT_DIR.glob('*.jsonl'):\n",
    "        shutil.copy2(f, DRIVE_DEST / f.name)\n",
    "        print(f\"   ‚úÖ {f.name} ‚Üí Drive\")\n",
    "    shutil.copy2(OUTPUT_DIR / 'stats.json', DRIVE_DEST / 'stats.json')\n",
    "    print(f\"\\nüìÅ Dataset en Drive: {DRIVE_DEST}\")\n",
    "else:\n",
    "    print(\"üìã Ejecutaste LOCALMENTE. Los archivos est√°n en:\")\n",
    "    print(f\"   {OUTPUT_DIR}\")\n",
    "    print()\n",
    "    print(\"   Para usar en FASE 3 (fine-tuning con Colab):\")\n",
    "    print(\"   1. Sube los JSONL a Google Drive > OKLA > dataset/\")\n",
    "    print(\"   2. O arrastra los archivos a Google Drive desde Finder\")\n",
    "    print()\n",
    "    for f in sorted(OUTPUT_DIR.glob('*.jsonl')):\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   üìÑ {f.name} ({size:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6fbe",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Resumen\n",
    "\n",
    "### Artefactos Generados\n",
    "| Archivo | Contenido |\n",
    "|---------|----------|\n",
    "| `okla_train.jsonl` | Datos de entrenamiento |\n",
    "| `okla_eval.jsonl` | Datos de evaluaci√≥n |\n",
    "| `okla_test.jsonl` | Datos de test |\n",
    "| `stats.json` | Estad√≠sticas del dataset |\n",
    "\n",
    "### üîú Siguiente: FASE 3\n",
    "Abre `FASE_3_TRAINING/okla_finetune_llama3.ipynb` ‚Üí Select Kernel ‚Üí Colab ‚Üí GPU T4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia 1.11",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
